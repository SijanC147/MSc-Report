% This file was created with Citavi 6.3.0.0

@proceedings{2002,
 year = {2002},
 title = {Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 series = {EMNLP '02}
}


@proceedings{2008,
 year = {2008},
 title = {Proceedings of the 25th International Conference on Machine Learning},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-60558-205-4},
 series = {ICML '08}
}


@proceedings{2010,
 year = {2010},
 title = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
 address = {USA},
 publisher = {Omnipress},
 isbn = {978-1-60558-907-7},
 series = {ICML'10}
}


@proceedings{2011,
 year = {2011},
 title = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 isbn = {978-1-932432-87-9},
 series = {HLT '11}
}


@proceedings{2011b,
 year = {2011},
 title = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 isbn = {978-1-937284-11-4},
 series = {EMNLP '11}
}


@proceedings{2011c,
 year = {2011},
 title = {Proceedings of the fourteenth international conference on artificial intelligence and statistics}
}


@proceedings{2013,
 year = {2013},
 title = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
 address = {USA},
 publisher = {{Curran Associates Inc}},
 series = {NIPS'13}
}


@proceedings{2013b,
 year = {2013},
 title = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
 publisher = {{Association for Computational Linguistics}}
}


@proceedings{2013c,
 year = {2013},
 title = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}
}


@proceedings{2014,
 year = {2014},
 title = {Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014)}
}


@proceedings{2014b,
 year = {2014},
 title = {NIPS Workshop on deep learning and representation learning}
}


@proceedings{2014c,
 year = {2014}
}


@proceedings{2015,
 year = {2015},
 title = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}
}


@proceedings{2015b,
 year = {2015},
 title = {Proceedings of the 24th International Conference on Artificial Intelligence},
 publisher = {{AAAI Press}},
 isbn = {978-1-57735-738-4},
 series = {IJCAI'15}
}


@proceedings{2015c,
 year = {2015},
 title = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
 publisher = {{AAAI Press}},
 isbn = {0-262-51129-0},
 series = {AAAI'15}
}


@proceedings{2015d,
 year = {2015},
 title = {International Conference on Machine Learning}
}


@proceedings{2015e,
 year = {2015},
 title = {Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015)}
}


@proceedings{2016,
 year = {2016},
 title = {Proceedings of the 10th International workshop on semantic evaluation (SemEval-2016)}
}


@proceedings{2016b,
 year = {2016},
 title = {Proceedings of the 10th international workshop on semantic evaluation (semeval-2016)}
}


@proceedings{2016c,
 year = {2016},
 title = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}
}


@proceedings{2016e,
 year = {2016},
 title = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
 publisher = {{The COLING 2016 Organizing Committee}}
}


@proceedings{2016g,
 year = {2016},
 title = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
 publisher = {{AAAI Press}},
 series = {AAAI'16}
}


@proceedings{2017,
 year = {2017},
 title = {Proceedings of the Twenty-Sixth International Joint Conference on  Artificial Intelligence, IJCAI-17}
}


@proceedings{2017b,
 year = {2017},
 title = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
 publisher = {JMLR.org},
 series = {ICML'17}
}


@proceedings{2017c,
 year = {2017},
 title = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}
}


@proceedings{2017d,
 year = {2017},
 title = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
 publisher = {{Association for Computational Linguistics}}
}


@proceedings{2017e,
 year = {2017},
 title = {Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)},
 address = {Vancouver, Canada},
 publisher = {{Association for Computational Linguistics}}
}


@proceedings{2017f,
 year = {2017},
 title = {Proceedings of the 11th international workshop on semantic evaluation (SemEval-2017)}
}


@proceedings{2017h,
 year = {2017},
 title = {WEBIST}
}


@proceedings{2018,
 year = {2018},
 title = {Proceedings of AAAI}
}


@proceedings{2018b,
 year = {2018},
 title = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}
}


@proceedings{2018c,
 year = {2018},
 title = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}
}


@article{ain2017,
 abstract = {The World Wide Web such as social networks, forums, review sites and blogs generate enormous heaps of data in the form of users views, emotions, opinions and arguments about different social events, products, brands, and politics. Sentiments of users that are expressed on the web has great influence on the readers, product vendors and politicians. The unstructured form of data from the social media is needed to be analyzed and well-structured and for this purpose, sentiment analysis has recognized significant attention. Sentiment analysis is referred as text organization that is used to classify the expressed mind-set or feelings in different manners such as negative, positive, favorable, unfavorable, thumbs up, thumbs down, etc. The challenge for sentiment analysis is lack of sufficient labeled data in the field of Natural Language Processing (NLP). And to solve this issue, the sentiment analysis and deep learning techniques have been merged because deep learning models are effective due to their automatic learning capability. This Review Paper highlights latest studies regarding the implementation of deep learning models such as deep neural networks, convolutional neural networks and many more for solving different problems of sentiment analysis such as sentiment classification, cross lingual problems, textual and visual analysis and product review analysis, etc.},
 author = {Ain, Qurat Tul and Ali, Mubashir and Riaz, Amna and Noureen, Amna and Kamran, Muhammad and Hayat, Babar and Rehman, A.},
 year = {2017},
 title = {Sentiment Analysis Using Deep Learning Techniques: A Review},
 url = {https://pdfs.semanticscholar.org/8892/24a64a5bc5f9e965f418a63b6768f7164993.pdf},
 pages = {424--433},
 volume = {8},
 number = {6},
 issn = {2158-107X},
 journal = {(IJACSA) International Journal of Advanced Computer Science and Applications},
 doi = {10.14569/IJACSA.2017.080657}
}


@proceedings{alessandromoschitti,
 year = {2014},
 title = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {{Alessandro Moschitti}, Qatar Computing Research Institute and {Bo Pang}, Google and {Walter Daelemans}, University of Antwerp},
 doi = {10.3115/v1/D14-1}
}


@article{alexgraves2014,
 author = {{Alex Graves} and {Greg Wayne} and {Ivo Danihelka}},
 year = {2014},
 title = {Neural Turing Machines},
 keywords = {Attention;Memory Networks},
 volume = {abs/1410.5401},
 journal = {CoRR}
}


@article{andrejkarpathy2014,
 author = {{Andrej Karpathy} and {Fei-Fei Li}},
 year = {2014},
 title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
 keywords = {Image Captioning},
 volume = {abs/1412.2306},
 journal = {CoRR}
}


@article{ankitkumar2015,
 author = {{Ankit Kumar} and {Ozan Irsoy} and {Jonathan Su} and {James Bradbury} and {Robert English} and {Brian Pierce} and {Peter Ondruska} and {Ishaan Gulrajani} and {Richard Socher}},
 year = {2015},
 title = {Ask Me Anything: Dynamic Memory Networks for Natural Language Processing},
 keywords = {Memory Networks},
 volume = {abs/1506.07285},
 journal = {CoRR}
}


@article{appel2016,
 author = {Appel, Orestes and Chiclana, Francisco and Carter, Jenny and Fujita, Hamido},
 year = {2016},
 title = {A hybrid approach to the sentiment analysis problem at the sentence level},
 pages = {110--124},
 volume = {108},
 journal = {Knowledge-Based Systems}
}


@article{bahdanau2014,
 abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
 author = {Bahdanau, Dzmitry and Cho, KyungHyun and Bengio, Yoshua},
 year = {2014},
 title = {Neural machine translation by jointly learning to align and translate},
 keywords = {Attention},
 journal = {arXiv preprint arXiv:1409.0473}
}


@proceedings{balahur,
 year = {2017},
 title = {Proceedings of the 8th Workshop on Computational Approaches to  Subjectivity, Sentiment and Social Media Analysis},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {Balahur, Alexandra and Mohammad, Saif M. and {van der Goot}, Erik},
 doi = {10.18653/v1/W17-52}
}


@inproceedings{barnes,
 author = {Barnes, Jeremy and Klinger, Roman and {Im Schulte Walde}, Sabine},
 title = {Assessing State-of-the-Art Sentiment Models on State-of-the-Art Sentiment Datasets},
 pages = {2--12},
 publisher = {{Association for Computational Linguistics}},
 editor = {Balahur, Alexandra and Mohammad, Saif M. and {van der Goot}, Erik},
 booktitle = {Proceedings of the 8th Workshop on Computational Approaches to  Subjectivity, Sentiment and Social Media Analysis},
 year = {2017},
 address = {Stroudsburg, PA, USA},
 doi = {10.18653/v1/W17-5202}
}


@inproceedings{baziotis2017,
 abstract = {In this paper we present two deep-learning systems that competed at SemEval-2017 Task 4 ``Sentiment Analysis in Twitter''. We participated in all subtasks for English tweets, involving message-level and topic-based sentiment polarity classification and quantification. We use Long Short-Term Memory (LSTM) networks augmented with two kinds of attention mechanisms, on top of word embeddings pre-trained on a big collection of Twitter messages. Also, we present a text processing tool suitable for social network messages, which performs tokenization, word normalization, segmentation and spell correction. Moreover, our approach uses no hand-crafted features or sentiment lexicons. We ranked 1st (tie) in Subtask A, and achieved very competitive results in the rest of the Subtasks. Both the word embeddings and our text processing tool are available to the research community.},
 author = {Baziotis, Christos and Pelekis, Nikos and Doulkeridis, Christos},
 title = {DataStories at SemEval-2017 Task 4: Deep LSTM with Attention for Message-level and Topic-based Sentiment Analysis},
 url = {https://www.aclweb.org/anthology/S17-2126},
 pages = {747--754},
 publisher = {{Association for Computational Linguistics}},
 booktitle = {Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)},
 year = {2017},
 address = {Vancouver, Canada},
 doi = {10.18653/v1/S17-2126}
}


@article{bengio1994,
 author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo and others},
 year = {1994},
 title = {Learning long-term dependencies with gradient descent is difficult},
 pages = {157--166},
 volume = {5},
 number = {2},
 journal = {IEEE transactions on neural networks}
}


@article{bengio2003,
 author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
 year = {2003},
 title = {A neural probabilistic language model},
 pages = {1137--1155},
 volume = {3},
 number = {Feb},
 journal = {Journal of machine learning research}
}


@proceedings{bethardb,
 year = {2016},
 title = {Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {Bethard, Steven and Carpuat, Marine and Cer, Daniel and Jurgens, David and Nakov, Preslav and Zesch, Torsten},
 doi = {10.18653/v1/S16-1}
}


@article{bhuwandhingra2017,
 author = {{Bhuwan Dhingra} and {Hanxiao Liu} and {Ruslan Salakhutdinov} and {William W. Cohen}},
 year = {2017},
 title = {A Comparative Study of Word Embeddings for Reading Comprehension},
 keywords = {OOV Literature},
 volume = {abs/1703.00993},
 journal = {CoRR}
}


@article{bojanowski2017,
 abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
 author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
 year = {2017},
 title = {Enriching Word Vectors with Subword Information},
 url = {http://aclweb.org/anthology/Q17-1010},
 keywords = {Word Embeddings},
 pages = {135--146},
 volume = {5},
 journal = {Transactions of the Association for Computational Linguistics}
}


@proceedings{bunnell1996,
 year = {1996},
 title = {ICSLP 96: Proceedings, fourth international conference on spoken language processing /   sponsored by University of Delaware, Alfred I. duPont Institute, in conjunction with Acoustical Society of America ... [et al.]},
 address = {New York},
 publisher = {{Institute of Electrical and Electronics Engineers}},
 isbn = {0-7803-3555-4},
 editor = {Bunnell, H. Timothy. Ed and Idsardi, William}
}


@book{c.cortes2015,
 year = {2015},
 title = {Advances in Neural Information Processing Systems 28},
 publisher = {{Curran Associates, Inc}},
 editor = {{C. Cortes} and {N. D. Lawrence} and {D. D. Lee} and {M. Sugiyama} and {R. Garnett}}
}


@inproceedings{cambria2015,
 author = {Cambria, Erik and Fu, Jie and Bisio, Federica and Poria, Soujanya},
 title = {AffectiveSpace 2: Enabling Affective Intuition for Concept-level Sentiment Analysis},
 url = {http://dl.acm.org/citation.cfm?id=2887007.2887078},
 pages = {508--514},
 publisher = {{AAAI Press}},
 isbn = {0-262-51129-0},
 series = {AAAI'15},
 booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
 year = {2015}
}


@inproceedings{chen2016,
 author = {Chen, Peng and Xu, Bing and Yang, Muyun and Li, Sheng},
 title = {Clause sentiment identification based on convolutional neural network with context embedding},
 pages = {1532--1538},
 booktitle = {2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
 year = {2016}
}


@inproceedings{chen2017,
 abstract = {We propose a novel framework based on neural networks to identify the sentiment of opinion targets in a comment/review. Our framework adopts multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. The results of multiple attentions are non-linearly combined with a recurrent neural network, which strengthens the expressive power of our model for handling more complications. The weightedmemory mechanism not only helps us avoid the labor-intensive feature engineering work, but also provides a tailor-made memory for different opinion targets of a sentence. We examine the merit of our model on four datasets: two are from SemEval2014, i.e. reviews of restaurants and laptops; a twitter dataset, for testing its performance on social media data; and a Chinese news comment dataset, for testing its language sensitivity. The experimental results show that our model consistently outperforms the state-of-the-art methods on different types of data.},
 author = {Chen, Peng and Sun, Zhongqian and Bing, Lidong and Yang, Wei},
 title = {Recurrent attention network on memory for aspect sentiment analysis},
 keywords = {Attention;GRU;Memory Networks},
 pages = {452--461},
 booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
 year = {2017}
}


@inproceedings{chen2017c,
 author = {Chen, Guibin and Ye, Deheng and Xing, Zhenchang and Chen, Jieshan and Cambria, Erik},
 title = {Ensemble application of convolutional and recurrent neural networks for multi-label text categorization},
 keywords = {Ensemble},
 pages = {2377--2383},
 publisher = {IEEE},
 isbn = {978-1-5090-6182-2},
 editor = {Networks, International Joint Conference on Neural},
 booktitle = {IJCNN 2017},
 year = {2017},
 address = {Piscataway, NJ},
 doi = {10.1109/IJCNN.2017.7966144}
}


@inproceedings{chen2018,
 abstract = {Recently, deep learning approaches have been widely used in language modeling and achieved great success. However, the out-of-vocabulary (OOV) words are often estimated in a rather crude way using only one special symbol, which ignores the linguistic information. In this paper we present an LSTM language model with structured word embeddings to tackle this problem. In our model, both input and output embeddings of LSTM language model are deployed with structured word embeddings. Utilizing syntactic-level and morphological-level parameters sharing, OOV words can be incorporated into the proposed model without retraining. The LSTM language model with structured word embeddings is instantiated for Chinese. Experiments show that the proposed model achieves PPL improvement on OOV words, and can be further integrated into automatic speech recognition systems for fast vocabulary updating.},
 author = {Chen, R. and Yu, K.},
 title = {Fast Oov Words Incorporation Using Structured Word Embeddings for Neural Network Language Model},
 keywords = {OOV Literature},
 pages = {6119--6123},
 booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 year = {2018},
 doi = {10.1109/ICASSP.2018.8461491}
}


@article{chung2014,
 author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
 year = {2014},
 title = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
 journal = {arXiv preprint arXiv:1412.3555}
}


@article{cliche2017,
 author = {{Mathieu Cliche}},
 year = {2017},
 title = {BB{\_}twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with  CNNs and LSTMs},
 keywords = {Ensemble},
 volume = {abs/1704.06125},
 journal = {CoRR}
}


@misc{colah-understanding-lstm,
 year = {12/12/2018},
 title = {Understanding LSTM Networks -- colah's blog},
 url = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
 urldate = {4/15/2019}
}


@inproceedings{collobert2008,
 author = {Collobert, Ronan and Weston, Jason},
 title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
 url = {http://doi.acm.org/10.1145/1390156.1390177},
 pages = {160--167},
 publisher = {ACM},
 isbn = {978-1-60558-205-4},
 series = {ICML '08},
 booktitle = {Proceedings of the 25th International Conference on Machine Learning},
 year = {2008},
 address = {New York, NY, USA},
 doi = {10.1145/1390156.1390177}
}


@article{collobert2011,
 author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
 year = {2011},
 title = {Natural language processing (almost) from scratch},
 pages = {2493--2537},
 volume = {12},
 number = {Aug},
 journal = {Journal of machine learning research}
}


@inproceedings{dauphin2017,
 author = {Dauphin, Yann N. and Fan, Angela and Auli, Michael and Grangier, David},
 title = {Language Modeling with Gated Convolutional Networks},
 url = {http://dl.acm.org/citation.cfm?id=3305381.3305478},
 pages = {933--941},
 publisher = {JMLR.org},
 series = {ICML'17},
 booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
 year = {2017}
}


@article{deerwester1990,
 author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
 year = {1990},
 title = {Indexing by latent semantic analysis},
 pages = {391--407},
 volume = {41},
 number = {6},
 journal = {Journal of the American society for information science}
}


@inproceedings{dehongma2017,
 abstract = {Aspect-level sentiment classification aims at identifying the sentiment polarity of specific target in its context. Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling thier contexts via generating target-specific representations. However, these studies always ignore the separate modeling of targets. In this paper, we argue that both targets and contexts deserve special treatment and need to be learned their own representations via interactive learning. Then, we propose the interactive attention networks (IAN) to interactively learn attentions in the contexts and targets, and generate the representations for targets and contexts separately. With this design, the IAN model can well represent a target and its collocative context, which is helpful to sentiment classification. Experimental results on SemEval 2014 Datasets demonstrate the effectiveness of our model.},
 author = {{Dehong Ma} and {Sujian Li} and {Xiaodong Zhang} and {Houfeng Wang}},
 title = {Interactive Attention Networks for Aspect-Level Sentiment Classification},
 keywords = {Attention},
 pages = {4068--4074},
 booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on  Artificial Intelligence, IJCAI-17},
 year = {2017},
 doi = {10.24963/ijcai.2017/568}
}


@proceedings{derijke2017,
 year = {2017},
 title = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {9781450346757},
 editor = {{de Rijke}, Maarten},
 institution = {{Association for Computing Machinery-Digital Library} and {ACM Special Interest Group on Management of Data} and {ACM Special Interest Group on Information Retrieval} and {ACM Special Interest Group on Hypertext, Hypermedia, and Web} and {ACM Special Interest Group on Knowledge Discovery in Data}},
 doi = {10.1145/3018661}
}


@inproceedings{dong,
 abstract = {We propose Adaptive Recursive Neural Network (AdaRNN) for target-dependent Twitter sentiment classification. AdaRNN adaptively propagates the sentiments of words to target depending on the context and syntactic relationships between them. It consists of more than one composition functions, and we model the adaptive sentiment propagations as distributions over these composition functions. The experimental studies illustrate that AdaRNN improves the baseline methods. Furthermore, we introduce a manually annotated dataset for target-dependent Twitter sentiment analysis.},
 author = {Dong, Li and Wei, Furu and Tan, Chuanqi and Tang, Duyu and Zhou, Ming and Xu, Ke},
 title = {Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification},
 pages = {49--54},
 publisher = {{Association for Computational Linguistics}},
 editor = {Toutanova, Kristina and Wu, Hua},
 booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 year = {2014},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/v1/P14-2009}
}


@incollection{dossantos2014,
 abstract = {Sentiment analysis of short texts such as single sentences and Twitter messages is challenging because of the limited contextual information that they normally contain. Effectively solving this task requires strategies that combine the small text content with prior knowledge and use more than just bag-of-words. In this work we propose a new deep convolutional neural network that ex-ploits from character-to sentence-level information to perform sentiment analysis of short texts. We apply our approach for two corpora of two different domains: the Stanford Sentiment Tree-bank (SSTb), which contains sentences from movie reviews; and the Stanford Twitter Sentiment corpus (STS), which contains Twitter messages. For the SSTb corpus, our approach achieves state-of-the-art results for single sentence sentiment prediction in both binary positive/negative classification, with 85.7{\%} accuracy, and fine-grained classification, with 48.3{\%} accuracy. For the STS corpus, our approach achieves a sentiment prediction accuracy of 86.4{\%}.},
 author = {{dos Santos}, Cicero and Gatti, Maira},
 title = {Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts},
 url = {http://www.aclweb.org/anthology/C14-1008},
 pages = {69--78},
 publisher = {{Dublin City University and Association for Computational Linguistics}},
 editor = {{Junichi Tsujii} and {Jan Hajic}},
 booktitle = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
 year = {2014},
 address = {Dublin, Ireland}
}


@article{frege1892,
 author = {Frege, Gottlob},
 year = {1892},
 title = {On sense and reference, 1892},
 pages = {563--583},
 journal = {Readings in the Philosophy of Language}
}


@inproceedings{gallwitz1996,
 abstract = {In almost all applications of automatic speech recognition, especially in spontaneous speech tasks, the recognizer vocabulary cannot cover all occurring words. There is always a significant amount of out-of-vocabulary words even when the vocabulary size is very large. We present a new approach for the integration of out-of-vocabulary words into statistical language models. We use category information for all words in the training corpus to define a function that gives an approximation of the out-of-vocabulary word emission probability for each word category. This information is integrated into the language models. Although we use a simple acoustic model for out-of-vocabulary words, we achieve a 6{\%} reduction of word error rate on spontaneous speech data with about 5{\%} out-of-vocabulary rate.},
 author = {Gallwitz, F. and Noth, E. and Niemann, H.},
 title = {A category based approach for recognition of out-of-vocabulary words},
 keywords = {OOV Literature},
 pages = {228--231},
 publisher = {{Institute of Electrical and Electronics Engineers}},
 isbn = {0-7803-3555-4},
 editor = {Bunnell, H. Timothy. Ed and Idsardi, William},
 booktitle = {ICSLP 96},
 year = {1996},
 address = {New York},
 doi = {10.1109/ICSLP.1996.607083}
}


@inproceedings{gers2000,
 author = {Gers, F. A.},
 title = {Learning to forget: continual prediction with LSTM},
 keywords = {LSTM},
 pages = {850--855},
 publisher = {IEE},
 isbn = {0 85296 721 7},
 series = {Conference publication / Institution of Electrical Engineers},
 booktitle = {ICANN 99, Ninth International Conference on Artificial Neural Networks},
 year = {19XX},
 address = {London},
 doi = {10.1049/cp:19991218}
}


@inproceedings{glorot2011,
 author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
 title = {Deep sparse rectifier neural networks},
 pages = {315--323},
 booktitle = {Proceedings of the fourteenth international conference on artificial intelligence and statistics},
 year = {2011}
}


@article{goldberg2015,
 author = {{Yoav Goldberg}},
 year = {2015},
 title = {A Primer on Neural Network Models for Natural Language Processing},
 volume = {abs/1510.00726},
 journal = {CoRR}
}


@book{goodfellow2016,
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
 year = {2016},
 title = {Deep learning},
 publisher = {{MIT press}}
}


@book{graves2012,
 year = {2012},
 title = {Supervised sequence labelling with recurrent neural networks},
 price = {{\pounds}90.00},
 keywords = {LSTM},
 address = {Heidelberg and London},
 volume = {v. 385},
 publisher = {Springer},
 isbn = {978-3-642-24796-5},
 series = {Studies in Computational Intelligence},
 editor = {Graves, Alex},
 doi = {10.1007/978-3-642-24797-2}
}


@incollection{graves2012b,
 author = {Graves, Alex},
 title = {Supervised Sequence Labelling},
 keywords = {LSTM},
 pages = {5--13},
 volume = {385},
 publisher = {Springer},
 isbn = {978-3-642-24796-5},
 series = {Studies in Computational Intelligence},
 editor = {Graves, Alex},
 booktitle = {Supervised sequence labelling with recurrent neural networks},
 year = {2012},
 address = {Heidelberg and London},
 doi = {10.1007/978-3-642-24797-2{\textunderscore }2}
}


@article{graves2016,
 author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'n}ska, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`a} Puigdom{\`e}nech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
 year = {2016},
 title = {Hybrid computing using a neural network with dynamic external memory},
 url = {https://doi.org/10.1038/nature20101},
 pages = {471  EP  -},
 volume = {538},
 journal = {Nature},
 doi = {10.1038/nature20101}
}


@article{hassabis2017,
 abstract = {The fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields.},
 author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
 year = {2017},
 title = {Neuroscience-Inspired Artificial Intelligence},
 keywords = {Animals;Artificial Intelligence;Attention;Brain/physiology;Humans;Neural Networks (Computer);Neurosciences},
 pages = {245--258},
 volume = {95},
 number = {2},
 journal = {Neuron},
 doi = {10.1016/j.neuron.2017.06.011}
}


@article{hermann2015,
 author = {{Karl Moritz Hermann} and {Tom{\'a}s Kocisk{\'y}} and {Edward Grefenstette} and {Lasse Espeholt} and {Will Kay} and {Mustafa Suleyman} and {Phil Blunsom}},
 year = {2015},
 title = {Teaching Machines to Read and Comprehend},
 keywords = {Attention},
 volume = {abs/1506.03340},
 journal = {CoRR}
}


@article{hinton2006,
 abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedbl}autoencoder{\textquotedbl} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
 author = {Hinton, G. E. and Salakhutdinov, R. R.},
 year = {2006},
 title = {Reducing the dimensionality of data with neural networks},
 pages = {504--507},
 volume = {313},
 number = {5786},
 journal = {Science (New York, N.Y.)},
 doi = {10.1126/science.1127647}
}


@article{hochreiter1997,
 abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
 author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
 year = {1997},
 title = {Long Short-Term Memory},
 keywords = {LSTM},
 pages = {1735--1780},
 volume = {9},
 number = {8},
 issn = {0899-7667},
 journal = {Neural Computation},
 doi = {10.1162/neco.1997.9.8.1735}
}


@proceedings{ieee2016,
 year = {2016},
 title = {2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
 institution = {IEEE}
}


@proceedings{institutionofelectricalengineers19XX,
 year = {19XX},
 title = {ICANN 99, Ninth International Conference on Artificial Neural Networks: [incorporating the IEE Conference on Artificial Neural Networks]; 7 - 10 September 1999 venue: University of Edinburgh UK},
 address = {London},
 volume = {470},
 publisher = {IEE},
 isbn = {0 85296 721 7},
 series = {Conference publication / Institution of Electrical Engineers},
 institution = {{Institution of Electrical Engineers} and DE-90 and {DE-90  LOK} and DE-90-148 and {DE-90-148  LOK}}
}


@inproceedings{jabreel2017,
 author = {Jabreel, Mohammed and Moreno, Antonio},
 title = {Target-dependent Sentiment Analysis of Tweets using a Bi-directional Gated Recurrent Unit},
 keywords = {GRU},
 pages = {80--87},
 booktitle = {WEBIST},
 year = {2017}
}


@article{jasonweston2014,
 abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
 author = {{Jason Weston} and {Sumit Chopra} and {Antoine Bordes}},
 year = {2014},
 title = {Memory Networks},
 url = {http://arxiv.org/abs/1410.3916},
 keywords = {Memory Networks},
 volume = {abs/1410.3916},
 journal = {CoRR}
}


@inproceedings{jiang2011,
 abstract = {Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-ofthe-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets). However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration. According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification.},
 author = {Jiang, Long and Yu, Mo and Zhou, Ming and Liu, Xiaohua and Zhao, Tiejun},
 title = {Target-dependent Twitter Sentiment Classification},
 url = {http://dl.acm.org/citation.cfm?id=2002472.2002492},
 pages = {151--160},
 publisher = {{Association for Computational Linguistics}},
 isbn = {978-1-932432-87-9},
 series = {HLT '11},
 booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
 year = {2011},
 address = {Stroudsburg, PA, USA}
}


@article{jianpengcheng2016,
 author = {{Jianpeng Cheng} and {Li Dong} and {Mirella Lapata}},
 year = {2016},
 title = {Long Short-Term Memory-Networks for Machine Reading},
 keywords = {Attention},
 volume = {abs/1601.06733},
 journal = {CoRR}
}


@article{jiweili2015,
 author = {{Jiwei Li} and {Dan Jurafsky} and {Eduard H. Hovy}},
 year = {2015},
 title = {When Are Tree Structures Necessary for Deep Learning of Representations?},
 keywords = {Representative Compositional Approaches},
 volume = {abs/1503.00185},
 journal = {CoRR}
}


@incollection{johnsonzhang2015,
 author = {Johnson, Rie and Zhang, Tong},
 title = {Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding},
 url = {http://papers.nips.cc/paper/5849-semi-supervised-convolutional-neural-networks-for-text-categorization-via-region-embedding.pdf},
 pages = {919--927},
 publisher = {{Curran Associates, Inc}},
 editor = {{C. Cortes} and {N. D. Lawrence} and {D. D. Lee} and {M. Sugiyama} and {R. Garnett}},
 booktitle = {Advances in Neural Information Processing Systems 28},
 year = {2015}
}


@book{junichitsujii2014,
 year = {2014},
 title = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
 url = {http://www.aclweb.org/anthology/C14-1},
 address = {Dublin, Ireland},
 publisher = {{Dublin City University and Association for Computational Linguistics}},
 editor = {{Junichi Tsujii} and {Jan Hajic}}
}


@article{junyoungchung2014,
 author = {{Junyoung Chung} and {{\c{C}}aglar G{\"u}l{\c{c}}ehre} and {Kyunghyun Cho} and {Yoshua Bengio}},
 year = {2014},
 title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence  Modeling},
 keywords = {GRU},
 volume = {abs/1412.3555},
 journal = {CoRR}
}


@article{kaishengtai2015,
 author = {{Kai Sheng Tai} and {Richard Socher} and {Christopher D. Manning}},
 year = {2015},
 title = {Improved Semantic Representations From Tree-Structured Long Short-Term  Memory Networks},
 keywords = {Tree Structured LSTM},
 volume = {abs/1503.00075},
 journal = {CoRR}
}


@article{kalchbrenner2014,
 author = {{Nal Kalchbrenner} and {Edward Grefenstette} and {Phil Blunsom}},
 year = {2014},
 title = {A Convolutional Neural Network for Modelling Sentences},
 volume = {abs/1404.2188},
 journal = {CoRR}
}


@article{kelvinxu2015,
 author = {{Kelvin Xu} and {Jimmy Ba} and {Ryan Kiros} and {Kyunghyun Cho} and {Aaron C. Courville} and {Ruslan Salakhutdinov} and {Richard S. Zemel} and {Yoshua Bengio}},
 year = {2015},
 title = {Show, Attend and Tell: Neural Image Caption Generation with Visual  Attention},
 keywords = {Attention},
 volume = {abs/1502.03044},
 journal = {CoRR}
}


@article{kharde2016,
 author = {{Vishal. A. Kharde} and {Sheetal. Sonawane}},
 year = {2016},
 title = {Sentiment Analysis of Twitter Data : A Survey of Techniques},
 volume = {abs/1601.06971},
 journal = {CoRR}
}


@inproceedings{kim,
 abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
 author = {Kim, Yoon},
 title = {Convolutional Neural Networks for Sentence Classification},
 pages = {1746--1751},
 publisher = {{Association for Computational Linguistics}},
 editor = {{Alessandro Moschitti}, Qatar Computing Research Institute and {Bo Pang}, Google and {Walter Daelemans}, University of Antwerp},
 booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 year = {2014},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/v1/D14-1181}
}


@article{kingma2014,
 author = {Kingma, Diederik P. and Ba, Jimmy},
 year = {2014},
 title = {Adam: A method for stochastic optimization},
 journal = {arXiv preprint arXiv:1412.6980}
}


@inproceedings{kiritchenko,
 abstract = {Reviews depict sentiments of customers towards various aspects of a product or service. Some of these aspects can be grouped into coarser aspect categories. SemEval-2014 had a shared task (Task 4) on aspect-level sentiment analysis, with over 30 teams participated. In this paper, we describe our submissions, which stood first in detecting aspect categories, first in detecting sentiment towards aspect categories, third in detecting aspect terms, and first and second in detecting sentiment towards aspect terms in the laptop and restaurant domains, respectively.},
 author = {Kiritchenko, Svetlana and Zhu, Xiaodan and Cherry, Colin and Mohammad, Saif},
 title = {NRC-Canada-2014: Detecting Aspects and Sentiment in Customer Reviews},
 pages = {437--442},
 publisher = {{Association for Computational Linguistics}},
 editor = {Nakov, Preslav and Zesch, Torsten},
 booktitle = {Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)},
 year = {2014},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/v1/S14-2076}
}


@article{kyunghyuncho2014b,
 author = {{Kyunghyun Cho} and {Bart van Merrienboer} and {{\c{C}}aglar G{\"u}l{\c{c}}ehre} and {Fethi Bougares} and {Holger Schwenk} and {Yoshua Bengio}},
 year = {2014},
 title = {Learning Phrase Representations using RNN Encoder-Decoder for Statistical  Machine Translation},
 keywords = {GRU},
 volume = {abs/1406.1078},
 journal = {CoRR}
}


@inproceedings{lakkaraju2014,
 author = {Lakkaraju, Himabindu and Socher, Richard and Manning, Chris},
 title = {Aspect specific sentiment analysis using hierarchical deep learning},
 booktitle = {NIPS Workshop on deep learning and representation learning},
 year = {2014}
}


@incollection{lecun1998,
 author = {LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
 title = {Efficient BackProp},
 pages = {9--48},
 volume = {7700},
 publisher = {Springer},
 isbn = {978-3-642-35288-1},
 series = {LNCS sublibrary. SL 1, Theoretical computer science and general issues},
 editor = {Montavon, Gr{\'e}goire and Orr, Genevieve and M{\"u}ller, Klaus-Robert},
 booktitle = {Neural networks},
 year = {2012},
 address = {Heidelberg},
 doi = {10.1007/978-3-642-35289-8{\textunderscore }3}
}


@inproceedings{lei2016,
 abstract = {Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications -- rationales -- that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.1},
 author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
 title = {Rationalizing Neural Predictions},
 pages = {107--117},
 publisher = {{Association for Computational Linguistics}},
 editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
 booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural  Language Processing},
 year = {2016},
 address = {Stroudsburg, PA, USA},
 doi = {10.18653/v1/D16-1011}
}


@inproceedings{li2017,
 abstract = {We consider the task of identifying attitudes towards a given set of entities from text. Conventionally, this task is decomposed into two separate subtasks: target detection that identifies whether each entity is mentioned in the text, either explicitly or implicitly, and polarity classification that classifies the exact sentiment towards an identified entity (the target) into positive, negative, or neutral.



Instead, we show that attitude identification can be solved with an end-to-end machine learning architecture, in which the two subtasks are interleaved by a deep memory network. In this way, signals produced in target detection provide clues for polarity classification, and reversely, the predicted polarity provides feedback to the identification of targets. Moreover, the treatments for the set of targets also influence each other -- the learned representations may share the same semantics for some targets but vary for others. The proposed deep memory network, the AttNet, outperforms methods that do not consider the interactions between the subtasks or those among the targets, including conventional machine learning methods and the state-of-the-art deep learning models.},
 author = {Li, Cheng and Guo, Xiaoxiao and Mei, Qiaozhu},
 title = {Deep Memory Networks for Attitude Identification},
 keywords = {Memory Networks},
 pages = {671--680},
 publisher = {ACM},
 isbn = {9781450346757},
 editor = {{de Rijke}, Maarten},
 booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
 year = {2017},
 address = {New York, NY},
 doi = {10.1145/3018661.3018714}
}


@proceedings{lim2017,
 year = {2017},
 title = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
 address = {New York, NY},
 publisher = {ACM},
 isbn = {9781450349185},
 editor = {Lim, Ee-Peng},
 institution = {{Association for Computing Machinery-Digital Library} and {ACM Special Interest Group on Hypertext, Hypermedia, and Web} and {ACM Special Interest Group on Information Retrieval}},
 doi = {10.1145/3132847}
}


@article{liu2012,
 author = {Liu, Bing},
 year = {2012},
 title = {Sentiment analysis and opinion mining},
 pages = {1--167},
 volume = {5},
 number = {1},
 journal = {Synthesis lectures on human language technologies}
}


@article{liu2018,
 abstract = {While neural networks have been shown to achieve impressive results for sentence-level sentiment analysis, targeted aspect-based sentiment analysis (TABSA) ---extraction of finegrained opinion polarity w.r.t. a pre-defined set of aspects --- remains a difficult task. Motivated by recent advances in memoryaugmented models for machine reading, we propose a novel architecture, utilising external ``memory chains'' with a delayed memory update mechanism to track entities. On a TABSA task, the proposed model demonstrates substantial improvements over state-ofthe-art approaches, including those using external knowledge bases.1},
 author = {Liu, Fei and Cohn, Trevor and Baldwin, Timothy},
 year = {2018},
 title = {Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-based Sentiment Analysis},
 keywords = {GRU;Memory Networks},
 journal = {arXiv preprint arXiv:1804.11019}
}


@inproceedings{ma2018,
 abstract = {Analyzing people's opinions and sentiments towards certain aspects is an important task of natural language understanding. In this paper, we propose a novel solution to targeted aspect-based sentiment analysis, which tackles the challenges of both aspect-based sentiment analysis and targeted sentiment analysis by exploiting commonsense knowledge. We augment the long short-term memory (LSTM) network with a hierarchical attention mechanism consisting of a target-level attention and a sentence-level attention. Commonsense knowledge of sentiment-related concepts is incorporated into the end-to-end training of a deep neural network for sentiment classification. In order to tightly integrate the commonsense knowledge into the recurrent encoder, we propose an extension of LSTM, termed Sentic LSTM. We conduct experiments on two publicly released datasets, which show that the combination of the proposed attention architecture and Sentic LSTM can outperform state-of-the-art methods in targeted aspect sentiment tasks.},
 author = {Ma, Yukun and Peng, Haiyun and Cambria, Erik},
 title = {Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive LSTM},
 url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16541/16152},
 pages = {5876--5883},
 booktitle = {Proceedings of AAAI},
 year = {2018}
}


@inproceedings{ma2018b,
 author = {Ma, Dehong and Li, Sujian and Wang, Houfeng},
 title = {Joint Learning for Targeted Sentiment Analysis},
 keywords = {GRU},
 pages = {4737--4742},
 booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
 year = {2018}
}


@article{manning2010,
 author = {Manning, Christopher and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
 year = {2010},
 title = {Introduction to information retrieval},
 pages = {100--103},
 volume = {16},
 number = {1},
 journal = {Natural Language Engineering}
}


@inproceedings{mikolov2013,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
 title = {Distributed Representations of Words and Phrases and Their Compositionality},
 url = {http://dl.acm.org/citation.cfm?id=2999792.2999959},
 keywords = {Word Embeddings},
 pages = {3111--3119},
 publisher = {{Curran Associates Inc}},
 series = {NIPS'13},
 booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
 year = {2013},
 address = {USA}
}


@article{mikolov2013b,
 author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
 year = {2013},
 title = {Efficient Estimation of Word Representations in Vector Space},
 keywords = {Word Embeddings},
 volume = {abs/1301.3781},
 journal = {CoRR}
}


@inproceedings{mikolov2013c,
 author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
 title = {Linguistic regularities in continuous space word representations},
 pages = {746--751},
 booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 year = {2013}
}


@article{minhthangluong2015,
 author = {{Minh-Thang Luong} and {Hieu Pham} and {Christopher D. Manning}},
 year = {2015},
 title = {Effective Approaches to Attention-based Neural Machine Translation},
 keywords = {Attention},
 volume = {abs/1508.04025},
 journal = {CoRR}
}


@inproceedings{mohammad,
 author = {Mohammad, Saif and Kiritchenko, Svetlana and Sobhani, Parinaz and Zhu, Xiaodan and Cherry, Colin},
 title = {SemEval-2016 Task 6: Detecting Stance in Tweets},
 pages = {31--41},
 publisher = {{Association for Computational Linguistics}},
 editor = {Bethard, Steven and Carpuat, Marine and Cer, Daniel and Jurgens, David and Nakov, Preslav and Zesch, Torsten},
 booktitle = {Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)},
 year = {2016},
 address = {Stroudsburg, PA, USA},
 doi = {10.18653/v1/S16-1003}
}


@book{montavon2012,
 year = {2012},
 title = {Neural networks: Tricks of the trade},
 address = {Heidelberg},
 edition = {2nd ed. /   Gr{\'e}goire Montavon, Genevi{\`e}ve B. Orr, Klaus-Robert M{\"u}ller},
 volume = {7700},
 publisher = {Springer},
 isbn = {978-3-642-35288-1},
 series = {LNCS sublibrary. SL 1, Theoretical computer science and general issues},
 editor = {Montavon, Gr{\'e}goire and Orr, Genevieve and M{\"u}ller, Klaus-Robert},
 doi = {10.1007/978-3-642-35289-8}
}


@article{moore2018,
 author = {{Andrew Moore} and {Paul Rayson}},
 year = {2018},
 title = {Bringing replication and reproduction together with generalisability  in NLP: Three reproduction studies for Target Dependent Sentiment  Analysis},
 volume = {abs/1806.05219},
 journal = {CoRR}
}


@inproceedings{nair2010,
 abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these {\textquotedbl}Stepped Sigmoid Units{\textquotedbl} are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
 author = {Nair, Vinod and Hinton, Geoffrey E.},
 title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
 url = {http://dl.acm.org/citation.cfm?id=3104322.3104425},
 pages = {807--814},
 publisher = {Omnipress},
 isbn = {978-1-60558-907-7},
 series = {ICML'10},
 booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
 year = {2010},
 address = {USA}
}


@proceedings{nakov,
 abstract = {Reviews depict sentiments of customers towards various aspects of a product or service. Some of these aspects can be grouped into coarser aspect categories. SemEval-2014 had a shared task (Task 4) on aspect-level sentiment analysis, with over 30 teams participated. In this paper, we describe our submissions, which stood first in detecting aspect categories, first in detecting sentiment towards aspect categories, third in detecting aspect terms, and first and second in detecting sentiment towards aspect terms in the laptop and restaurant domains, respectively.},
 year = {2014},
 title = {Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {Nakov, Preslav and Zesch, Torsten},
 doi = {10.3115/v1/S14-2}
}


@inproceedings{nakov2016,
 author = {Nakov, Preslav and Ritter, Alan and Rosenthal, Sara and Sebastiani, Fabrizio and Stoyanov, Veselin},
 title = {SemEval-2016 task 4: Sentiment analysis in Twitter},
 pages = {1--18},
 booktitle = {Proceedings of the 10th international workshop on semantic evaluation (semeval-2016)},
 year = {2016}
}


@proceedings{nakovb,
 year = {2015},
 title = {Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {Nakov, Preslav and Zesch, Torsten and Cer, Daniel and Jurgens, David},
 doi = {10.18653/v1/S15-2}
}


@article{naptali2012,
 author = {NAPTALI, Welly and TSUCHIYA, Masatoshi and NAKAGAWA, Seiichi},
 year = {2012},
 title = {Class-Based N-Gram Language Model for New Words Using Out-of-Vocabulary to In-Vocabulary Similarity},
 keywords = {OOV Literature},
 pages = {2308--2317},
 volume = {E95.D},
 number = {9},
 issn = {0916-8532},
 journal = {IEICE Transactions on Information and Systems},
 doi = {10.1587/transinf.E95.D.2308}
}


@proceedings{networks2017,
 year = {2017},
 title = {IJCNN 2017: The International Joint Conference on Neural Networks},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5090-6182-2},
 editor = {Networks, International Joint Conference on Neural},
 institution = {{International Joint Conference on Neural Networks} and IJCNN}
}


@inproceedings{nguyen2015,
 author = {Nguyen, Thien Hai and Shirai, Kiyoaki},
 title = {Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis},
 pages = {2509--2514},
 booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
 year = {2015}
}


@inproceedings{pang2002,
 author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
 title = {Thumbs Up?: Sentiment Classification Using Machine Learning Techniques},
 url = {https://doi.org/10.3115/1118693.1118704},
 pages = {79--86},
 publisher = {{Association for Computational Linguistics}},
 series = {EMNLP '02},
 booktitle = {Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10},
 year = {2002},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/1118693.1118704}
}


@article{pang2008,
 author = {Pang, Bo and Lee, Lillian and others},
 year = {2008},
 title = {Opinion mining and sentiment analysis},
 pages = {1--135},
 volume = {2},
 number = {1--2},
 journal = {Foundations and Trends$\backslash$textregistered in Information Retrieval}
}


@inproceedings{pennington,
 author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
 title = {Glove: Global Vectors for Word Representation},
 keywords = {Word Embeddings},
 pages = {1532--1543},
 publisher = {{Association for Computational Linguistics}},
 editor = {{Alessandro Moschitti}, Qatar Computing Research Institute and {Bo Pang}, Google and {Walter Daelemans}, University of Antwerp},
 booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 year = {2014},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/v1/D14-1162}
}


@inproceedings{pontiki,
 author = {Pontiki, Maria and Galanis, Dimitris and Pavlopoulos, John and Papageorgiou, Harris and Androutsopoulos, Ion and Manandhar, Suresh},
 title = {SemEval-2014 Task 4: Aspect Based Sentiment Analysis},
 pages = {27--35},
 publisher = {{Association for Computational Linguistics}},
 editor = {Nakov, Preslav and Zesch, Torsten},
 booktitle = {Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)},
 year = {2014},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/v1/S14-2004}
}


@inproceedings{pontikib,
 abstract = {SemEval-2015 Task 12, a continuation of SemEval-2014 Task 4, aimed to foster research beyond sentence- or text-level sentiment classification towards Aspect Based Sentiment Analysis. The goal is to identify opinions expressed about specific entities (e.g., laptops) and their aspects (e.g., price). The task provided manually annotated reviews in three domains (restaurants, laptops and hotels), and a common evaluation procedure. It attracted 93 submissions from 16 teams.},
 author = {Pontiki, Maria and Galanis, Dimitris and Papageorgiou, Haris and Manandhar, Suresh and Androutsopoulos, Ion},
 title = {SemEval-2015 Task 12: Aspect Based Sentiment Analysis},
 pages = {486--495},
 publisher = {{Association for Computational Linguistics}},
 editor = {Nakov, Preslav and Zesch, Torsten and Cer, Daniel and Jurgens, David},
 booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)},
 year = {2015},
 address = {Stroudsburg, PA, USA},
 doi = {10.18653/v1/S15-2082}
}


@article{poria2016,
 author = {{Soujanya Poria} and {Erik Cambria} and {Devamanyu Hazarika} and {Prateek Vij}},
 year = {2016},
 title = {A Deeper Look into Sarcastic Tweets Using Deep Convolutional Neural  Networks},
 volume = {abs/1610.08815},
 journal = {CoRR}
}


@article{reimers2017,
 author = {{Nils Reimers} and {Iryna Gurevych}},
 year = {2017},
 title = {Reporting Score Distributions Makes a Difference: Performance Study  of LSTM-networks for Sequence Tagging},
 volume = {abs/1707.09861},
 journal = {CoRR}
}


@inproceedings{rosenthal2014,
 author = {Rosenthal, Sara and Ritter, Alan and Nakov, Preslav and Stoyanov, Veselin},
 title = {SemEval-2014 Task 9: Sentiment Analysis in Twitter},
 pages = {73--80},
 year = {2014},
 doi = {10.3115/v1/S14-2009}
}


@inproceedings{rosenthal2015,
 author = {Rosenthal, Sara and Nakov, Preslav and Kiritchenko, Svetlana and Mohammad, Saif and Ritter, Alan and Stoyanov, Veselin},
 title = {Semeval-2015 task 10: Sentiment analysis in twitter},
 pages = {451--463},
 booktitle = {Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015)},
 year = {2015}
}


@inproceedings{rosenthal2017,
 author = {Rosenthal, Sara and Farra, Noura and Nakov, Preslav},
 title = {SemEval-2017 task 4: Sentiment analysis in Twitter},
 pages = {502--518},
 booktitle = {Proceedings of the 11th international workshop on semantic evaluation (SemEval-2017)},
 year = {2017}
}


@article{rumelhart1988,
 author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J. and others},
 year = {1988},
 title = {Learning representations by back-propagating errors},
 pages = {1},
 volume = {5},
 number = {3},
 journal = {Cognitive modeling}
}


@inproceedings{saeidi2016,
 abstract = {In this paper, we introduce the task of targeted aspect-based sentiment analysis. The goal is to extract fine-grained information with respect to entities mentioned in user comments. This work extends both aspect-based sentiment analysis that assumes a single entity per document and targeted sentiment analysis that assumes a single sentiment towards a target entity. In particular, we identify the sentiment towards each aspect of one or more entities. As a testbed for this task, we introduce the SentiHood dataset, extracted from a question answering (QA) platform where urban neighbourhoods are discussed by users. In this context units of text often mention several aspects of one or more neighbourhoods. This is the first time that a generic social media platform in this case a QA platform, is used for fine-grained opinion mining. Text coming from QA platforms is far less constrained compared to text from review specific platforms which current datasets are based on. We develop several strong baselines, relying on logistic regression and state-of-the-art recurrent neural networks.},
 author = {Saeidi, Marzieh and Bouchard, Guillaume and Liakata, Maria and Riedel, Sebastian},
 title = {SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods},
 url = {http://www.aclweb.org/anthology/C16-1146},
 pages = {1546--1556},
 publisher = {{The COLING 2016 Organizing Committee}},
 booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
 year = {2016}
}


@article{singh2016,
 author = {Singh, Mittul and Greenberg, Clayton and Oualil, Youssef and Klakow, Dietrich},
 year = {2016},
 title = {Sub-Word Similarity based Search for Embeddings: Inducing Rare-Word Embeddings for Word Similarity Tasks and Language Modelling},
 url = {http://www.aclweb.org/anthology/C16-1194},
 keywords = {OOV Literature},
 pages = {2061--2070},
 journal = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers}
}


@inproceedings{socher2011,
 author = {Socher, Richard and Pennington, Jeffrey and Huang, Eric H. and Ng, Andrew Y. and Manning, Christopher D.},
 title = {Semi-supervised Recursive Autoencoders for Predicting Sentiment Distributions},
 url = {http://dl.acm.org/citation.cfm?id=2145432.2145450},
 pages = {151--161},
 publisher = {{Association for Computational Linguistics}},
 isbn = {978-1-937284-11-4},
 series = {EMNLP '11},
 booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
 year = {2011},
 address = {Stroudsburg, PA, USA}
}


@inproceedings{socher2013,
 author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
 title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
 url = {http://www.aclweb.org/anthology/D13-1170},
 pages = {1631--1642},
 publisher = {{Association for Computational Linguistics}},
 booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
 year = {2013}
}


@article{srivastava2014,
 abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different {\textquotedbl}thinned{\textquotedbl} networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
 author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
 year = {2014},
 title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
 url = {http://dl.acm.org/citation.cfm?id=2627435.2670313},
 pages = {1929--1958},
 volume = {15},
 number = {1},
 issn = {1532-4435},
 journal = {J. Mach. Learn. Res.}
}


@inproceedings{stojanovski2016,
 author = {Stojanovski, Dario and Strezoski, Gjorgji and Madjarov, Gjorgji and Dimitrovski, Ivica},
 title = {Finki at semeval-2016 task 4: Deep learning architecture for twitter sentiment analysis},
 keywords = {Ensemble},
 pages = {149--154},
 booktitle = {Proceedings of the 10th International workshop on semantic evaluation (SemEval-2016)},
 year = {2016}
}


@proceedings{su,
 year = {2016},
 title = {Proceedings of the 2016 Conference on Empirical Methods in Natural  Language Processing},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
 doi = {10.18653/v1/D16-1}
}


@incollection{sukhbaatar2015,
 abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
 author = {Sukhbaatar, Sainbayar and szlam, arthur and Weston, Jason and Fergus, Rob},
 title = {End-To-End Memory Networks},
 url = {http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf},
 keywords = {Memory Networks},
 pages = {2440--2448},
 publisher = {{Curran Associates, Inc}},
 editor = {{C. Cortes} and {N. D. Lawrence} and {D. D. Lee} and {M. Sugiyama} and {R. Garnett}},
 booktitle = {Advances in Neural Information Processing Systems 28},
 year = {2015}
}


@inproceedings{tang,
 author = {Tang, Duyu and Wei, Furu and Yang, Nan and Zhou, Ming and Liu, Ting and Qin, Bing},
 title = {Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification},
 pages = {1555--1565},
 publisher = {{Association for Computational Linguistics}},
 editor = {Toutanova, Kristina and Wu, Hua},
 booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 year = {2014},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/v1/P14-1146}
}


@inproceedings{tang2016,
 abstract = {We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.},
 author = {Tang, Duyu and Qin, Bing and Liu, Ting},
 title = {Aspect Level Sentiment Classification with Deep Memory Network},
 keywords = {Memory Networks},
 pages = {214--224},
 publisher = {{Association for Computational Linguistics}},
 editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
 booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural  Language Processing},
 year = {2016},
 address = {Stroudsburg, PA, USA},
 doi = {10.18653/v1/D16-1021}
}


@inproceedings{tang2016b,
 abstract = {Target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence representation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.},
 author = {Tang, Duyu and Qin, Bing and Feng, Xiaocheng and Liu, Ting},
 title = {Effective LSTMs for Target-Dependent Sentiment Classification},
 url = {http://www.aclweb.org/anthology/C16-1311},
 pages = {3298--3307},
 publisher = {{The COLING 2016 Organizing Committee}},
 booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
 year = {2016}
}


@inproceedings{tay2017,
 abstract = {This paper proposes Dyadic Memory Networks (DyMemNN), a novel extension of end-to-end memory networks (memNN) for aspect-based sentiment analysis (ABSA). Originally designed for question answering tasks, memNN operates via a memory selection operation in which relevant memory pieces are adaptively selected based on the input query. In the problem of ABSA, this is analogous to aspects and documents in which the relationship between each word in the document is compared with the aspect vector. In the standard memory networks, simple dot products or feed forward neural networks are used to model the relationship between aspect and words which lacks representation learning capability. As such, our dyadic memory networks ameliorates this weakness by enabling rich dyadic interactions between aspect and word embeddings by integrating either parameterized neural tensor compositions or holographic compositions into the memory selection operation. To this end, we propose two variations of our dyadic memory networks, namely the Tensor DyMemNN and Holo DyMemNN. Overall, our two models are end-to-end neural architectures that enable rich dyadic interaction between aspect and document which intuitively leads to better performance. Via extensive experiments, we show that our proposed models achieve the state-of-the-art performance and outperform many neural architectures across six benchmark datasets.},
 author = {Tay, Yi and Tuan, Luu Anh and Hui, Siu Cheung},
 title = {Dyadic Memory Networks for Aspect-based Sentiment Analysis},
 keywords = {Memory Networks},
 pages = {107--116},
 publisher = {ACM},
 isbn = {9781450349185},
 editor = {Lim, Ee-Peng},
 booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
 year = {2017},
 address = {New York, NY},
 doi = {10.1145/3132847.3132936}
}


@proceedings{toutanova,
 year = {2014},
 title = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {Toutanova, Kristina and Wu, Hua},
 doi = {10.3115/v1/P14-1}
}


@proceedings{toutanovab,
 year = {2014},
 title = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {Toutanova, Kristina and Wu, Hua},
 doi = {10.3115/v1/P14-2}
}


@inproceedings{vo2015,
 abstract = {Target-dependent sentiment analysis on Twitter has attracted increasing research attention. Most previous work relies on syntax, such as automatic parse trees, which are subject to noise for informal text such as tweets. In this paper, we show that competitive results can be achieved without the use of syntax, by extracting a rich set of automatic features. In particular, we split a tweet into a left context and a right context according to a given target, using distributed word representations and neural pooling functions to extract features. Both sentiment-driven and standard embeddings are used, and a rich set of neural pooling functions are explored. Sentiment lexicons are used as an additional source of information for feature extraction. In standard evaluation, the conceptually simple method gives a 4.8{\%} absolute improvement over the state-of-the-art on three-way targeted sentiment classification, achieving the best reported results for this task.},
 author = {Vo, Duy-Tin and Zhang, Yue},
 title = {Target-dependent Twitter Sentiment Classification with Rich Automatic Features},
 url = {http://dl.acm.org/citation.cfm?id=2832415.2832437},
 pages = {1347--1353},
 publisher = {{AAAI Press}},
 isbn = {978-1-57735-738-4},
 series = {IJCAI'15},
 booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
 year = {2015}
}


@inproceedings{wagner2014,
 author = {Wagner, Joachim and Arora, Piyush and Cortes, Santiago and Barman, Utsab and Bogdanova, Dasha and Foster, Jennifer and Tounsi, Lamia},
 title = {Dcu: Aspect-based polarity classification for semeval task 4},
 pages = {223--229},
 booktitle = {Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014)},
 year = {2014}
}


@inproceedings{wang,
 abstract = {Aspect-level sentiment classification is a finegrained task in sentiment analysis. Since it provides more complete and in-depth results, aspect-level sentiment analysis has received much attention these years. In this paper, we reveal that the sentiment polarity of a sentence is not only determined by the content but is also highly related to the concerned aspect. For instance, ``The appetizers are ok, but the service is slow.'', for aspect taste, the polarity is positive while for service, the polarity is negative. Therefore, it is worthwhile to explore the connection between an aspect and the content of a sentence. To this end, we propose an Attention-based Long Short-Term Memory Network for aspect-level sentiment classification. The attention mechanism can concentrate on different parts of a sentence when different aspects are taken as input. We experiment on the SemEval 2014 dataset and results show that our model achieves state-ofthe-art performance on aspect-level sentiment classification.},
 author = {Wang, Yequan and Huang, Minlie and zhu, xiaoyan and Zhao, Li},
 title = {Attention-based LSTM for Aspect-level Sentiment Classification},
 keywords = {Attention},
 pages = {606--615},
 publisher = {{Association for Computational Linguistics}},
 editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
 booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural  Language Processing},
 year = {2016},
 address = {Stroudsburg, PA, USA},
 doi = {10.18653/v1/D16-1058}
}


@inproceedings{wang2017,
 abstract = {Existing target-specific sentiment recognition methods consider only a single target per tweet, and have been shown to miss nearly half of the actual targets mentioned. We present a corpus of UK election tweets, with an average of 3.09 entities per tweet and more than one type of sentiment in half of the tweets. This requires a method for multi-target specific sentiment recognition, which we develop by using the context around a target as well as syntactic dependencies involving the target. We present results of our method on both a benchmark corpus of single targets and the multi-target election corpus, showing state-of-the art performance in both corpora and outperforming previous approaches to multi-target sentiment task as well as deep learning models for singletarget sentiment.},
 author = {Wang, Bo and Liakata, Maria and Zubiaga, Arkaitz and Procter, Rob},
 title = {TDParse: Multi-target-specific sentiment recognition on Twitter},
 url = {http://aclweb.org/anthology/E17-1046},
 pages = {483--493},
 publisher = {{Association for Computational Linguistics}},
 booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
 year = {2017}
}


@article{wang2018,
 abstract = {Aspect sentiment classification (ASC) is a fundamental task in sentiment analysis. Given an aspect/target and a sentence, the task classifies the sentiment polarity expressed on the target in the sentence. Memory networks (MNs) have been used for this task recently and have achieved state-of-the-art results. In MNs, attention mechanism plays a crucial role in detecting the sentiment context for the given target. However, we found an important problem with the current MNs in performing the ASC task. Simply improving the attention mechanism will not solve it. The problem is referred to as target-sensitive sentiment, which means that the sentiment polarity of the (detected) context is dependent on the given target and it cannot be inferred from the context alone. To tackle this problem, we propose the targetsensitive memory networks (TMNs). Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated.},
 author = {Wang, Shuai and Mazumder, Sahisnu and Liu, Bing and Zhou, Mianwei and Chang, Yi},
 year = {2018},
 title = {Target-Sensitive Memory Networks for Aspect Sentiment Classification},
 url = {http://www.aclweb.org/anthology/P18-1088},
 keywords = {Memory Networks},
 pages = {957--967},
 volume = {1},
 journal = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}
}


@article{xinli2018,
 author = {{Xin Li} and {Lidong Bing} and {Wai Lam} and {Bei Shi}},
 year = {2018},
 title = {Transformation Networks for Target-Oriented Sentiment Classification},
 keywords = {Ensemble},
 volume = {abs/1805.01086},
 journal = {CoRR}
}


@article{xue2018,
 abstract = {Aspect based sentiment analysis (ABSA) can provide more detailed information than general sentiment analysis, because it aims to predict the sentiment polarities of the given aspects or entities in text. We summarize previous approaches into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). Most previous approaches employ long short-term memory and attention mechanisms to predict the sentiment polarity of the concerned targets, which are often complicated and need more training time. We propose a model based on convolutional neural networks and gating mechanisms, which is more accurate and efficient. First, the novel Gated Tanh-ReLU Units can selectively output the sentiment features according to the given aspect or entity. The architecture is much simpler than attention layer used in the existing models. Second, the computations of our model could be easily parallelized during training, because convolutional layers do not have time dependency as in LSTM layers, and gating units also work independently. The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models.},
 author = {Xue, Wei and Li, Tao},
 year = {2018},
 title = {Aspect Based Sentiment Analysis with Gated Convolutional Networks},
 keywords = {Ensemble;GRU},
 journal = {arXiv preprint arXiv:1805.07043}
}


@inproceedings{yang2016,
 abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
 author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
 title = {Hierarchical attention networks for document classification},
 pages = {1480--1489},
 booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 year = {2016}
}


@article{yezhang2015,
 author = {{Ye Zhang} and {Byron C. Wallace}},
 year = {2015},
 title = {A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional  Neural Networks for Sentence Classification},
 volume = {abs/1510.03820},
 journal = {CoRR}
}


@article{young2017,
 abstract = {Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.},
 author = {{Tom Young} and {Devamanyu Hazarika} and {Soujanya Poria} and {Erik Cambria}},
 year = {2017},
 title = {Recent Trends in Deep Learning Based Natural Language Processing},
 pages = {1--24},
 volume = {abs/1708.02709},
 journal = {CoRR}
}


@article{yuvalpinter2017,
 author = {{Yuval Pinter} and {Robert Guthrie} and {Jacob Eisenstein}},
 year = {2017},
 title = {Mimicking Word Embeddings using Subword RNNs},
 volume = {abs/1707.06961},
 journal = {CoRR}
}


@inproceedings{zhang2016,
 abstract = {Targeted sentiment analysis classifies the sentiment polarity towards each target entity mention in given text documents. Seminal methods extract manual discrete features from automatic syntactic parse trees in order to capture semantic information of the enclosing sentence with respect to a target entity mention. Recently, it has been shown that competitive accuracies can be achieved without using syntactic parsers, which can be highly inaccurate on noisy text such as tweets. This is achieved by applying distributed word representations and rich neural pooling functions over a simple and intuitive segmentation of tweets according to target entity mentions. In this paper, we extend this idea by proposing a sentencelevel neural model to address the limitation of pooling functions, which do not explicitly model tweet-level semantics. First, a bi-directional gated neural network is used to connect the words in a tweet so that pooling functions can be applied over the hidden layer instead of words for better representing the target and its contexts. Second, a three-way gated neural network structure is used to model the interaction between the target mention and its surrounding contexts. Experiments show that our proposed model gives significantly higher accuracies compared to the current best method for targeted sentiment analysis.},
 author = {Zhang, Meishan and Zhang, Yue and Vo, Duy-Tin},
 title = {Gated Neural Networks for Targeted Sentiment Analysis},
 url = {http://dl.acm.org/citation.cfm?id=3016100.3016334},
 pages = {3087--3093},
 publisher = {{AAAI Press}},
 series = {AAAI'16},
 booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
 year = {2016}
}


@article{zhang2018,
 abstract = {Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. Along with the success of deep learning in many application domains, deep learning is also used in sentiment analysis in recent years. This paper gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis. This article is categorized under: Fundamental Concepts of Data and Knowledge {\textgreater} Data Concepts Algorithmic Development {\textgreater} Text Mining},
 author = {Zhang, Lei and Wang, Shuai and Liu, Bing},
 year = {2018},
 title = {Deep learning for sentiment analysis: A survey},
 pages = {e1253},
 volume = {8},
 number = {4},
 issn = {1942-4787},
 journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
 doi = {10.1002/widm.1253}
}


@article{zheng2018,
 abstract = {Deep learning techniques have achieved success in aspect-based sentiment analysis in recent years. However, there are two important issues that still remain to be further studied, i.e., 1) how to efficiently represent the target especially when the target contains multiple words; 2) how to utilize the interaction between target and left/right contexts to capture the most important words in them. In this paper, we propose an approach, called left-centerright separated neural network with rotatory attention (LCR-Rot), to better address the two problems. Our approach has two characteristics: 1) it has three separated LSTMs, i.e., left, center and right LSTMs, corresponding to three parts of a review (left context, target phrase and right context); 2) it has a rotatory attention mechanism which models the relation between target and left/right contexts. The target2context attention is used to capture the most indicative sentiment words in left/right contexts. Subsequently, the context2target attention is used to capture the most important word in the target. This leads to a two-side representation of the target: left-aware target and right-aware target. We compare our approach on three benchmark datasets with ten related methods proposed recently. The results show that our approach significantly outperforms the state-of-the-art techniques.},
 author = {Zheng, Shiliang and Xia, Rui},
 year = {2018},
 title = {Left-Center-Right Separated Neural Network for Aspect-based Sentiment Analysis with Rotatory Attention},
 keywords = {Attention},
 journal = {arXiv preprint arXiv:1802.00892}
}


@inproceedings{zhu2015,
 author = {Zhu, Xiaodan and Sobihani, Parinaz and Guo, Hongyu},
 title = {Long short-term memory over recursive structures},
 keywords = {Tree Structured LSTM},
 pages = {1604--1612},
 booktitle = {International Conference on Machine Learning},
 year = {2015}
}


