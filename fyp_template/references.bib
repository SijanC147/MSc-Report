% This file was created with Citavi 6.3.0.0

@proceedings{2002,
 year = {2002},
 title = {Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 series = {EMNLP '02}
}


@proceedings{2008,
 year = {2008},
 title = {Proceedings of the 25th International Conference on Machine Learning},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-60558-205-4},
 series = {ICML '08}
}


@proceedings{2011,
 year = {2011},
 title = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 isbn = {978-1-932432-87-9},
 series = {HLT '11}
}


@proceedings{2011b,
 year = {2011},
 title = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 isbn = {978-1-937284-11-4},
 series = {EMNLP '11}
}


@proceedings{2013,
 year = {2013},
 title = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
 address = {USA},
 publisher = {{Curran Associates Inc}},
 series = {NIPS'13}
}


@proceedings{2013b,
 year = {2013},
 title = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
 publisher = {{Association for Computational Linguistics}}
}


@proceedings{2014,
 year = {2014},
 title = {Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014)}
}


@proceedings{2015,
 year = {2015},
 title = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}
}


@proceedings{2015b,
 year = {2015},
 title = {Proceedings of the 24th International Conference on Artificial Intelligence},
 publisher = {{AAAI Press}},
 isbn = {978-1-57735-738-4},
 series = {IJCAI'15}
}


@proceedings{2016,
 year = {2016},
 title = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
 publisher = {{AAAI Press}},
 series = {AAAI'16}
}


@proceedings{2016b,
 year = {2016},
 title = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}
}


@proceedings{2016c,
 year = {2016},
 title = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
 publisher = {{The COLING 2016 Organizing Committee}}
}


@proceedings{2017,
 year = {2017},
 title = {Proceedings of the Twenty-Sixth International Joint Conference on  Artificial Intelligence, IJCAI-17}
}


@proceedings{2017c,
 year = {2017},
 title = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}
}


@proceedings{2018c,
 year = {2018},
 title = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}
}


@proceedings{alessandromoschitti,
 year = {2014},
 title = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {{Alessandro Moschitti}, Qatar Computing Research Institute and {Bo Pang}, Google and {Walter Daelemans}, University of Antwerp},
 doi = {10.3115/v1/D14-1}
}


@article{ankitkumar2015,
 author = {{Ankit Kumar} and {Ozan Irsoy} and {Jonathan Su} and {James Bradbury} and {Robert English} and {Brian Pierce} and {Peter Ondruska} and {Ishaan Gulrajani} and {Richard Socher}},
 year = {2015},
 title = {Ask Me Anything: Dynamic Memory Networks for Natural Language Processing},
 volume = {abs/1506.07285},
 journal = {CoRR}
}


@article{appel2016,
 author = {Appel, Orestes and Chiclana, Francisco and Carter, Jenny and Fujita, Hamido},
 year = {2016},
 title = {A hybrid approach to the sentiment analysis problem at the sentence level},
 pages = {110--124},
 volume = {108},
 journal = {Knowledge-Based Systems}
}


@article{bahdanau2014,
 abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
 author = {Bahdanau, Dzmitry and Cho, KyungHyun and Bengio, Yoshua},
 year = {2014},
 title = {Neural machine translation by jointly learning to align and translate},
 journal = {arXiv preprint arXiv:1409.0473}
}


@article{bengio2003,
 author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
 year = {2003},
 title = {A neural probabilistic language model},
 pages = {1137--1155},
 volume = {3},
 number = {Feb},
 journal = {Journal of machine learning research}
}


@article{bhuwandhingra2017,
 author = {{Bhuwan Dhingra} and {Hanxiao Liu} and {Ruslan Salakhutdinov} and {William W. Cohen}},
 year = {2017},
 title = {A Comparative Study of Word Embeddings for Reading Comprehension},
 keywords = {OOV Literature},
 volume = {abs/1703.00993},
 journal = {CoRR}
}


@article{bojanowski2017,
 abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
 author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
 year = {2017},
 title = {Enriching Word Vectors with Subword Information},
 url = {http://aclweb.org/anthology/Q17-1010},
 keywords = {Word Embeddings},
 pages = {135--146},
 volume = {5},
 journal = {Transactions of the Association for Computational Linguistics}
}


@proceedings{bunnell1996,
 year = {1996},
 title = {ICSLP 96: Proceedings, fourth international conference on spoken language processing /   sponsored by University of Delaware, Alfred I. duPont Institute, in conjunction with Acoustical Society of America ... [et al.]},
 address = {New York},
 publisher = {{Institute of Electrical and Electronics Engineers}},
 isbn = {0-7803-3555-4},
 editor = {Bunnell, H. Timothy. Ed and Idsardi, William}
}


@book{c.cortes2015,
 year = {2015},
 title = {Advances in Neural Information Processing Systems 28},
 publisher = {{Curran Associates, Inc}},
 editor = {{C. Cortes} and {N. D. Lawrence} and {D. D. Lee} and {M. Sugiyama} and {R. Garnett}}
}


@inproceedings{chen2017,
 abstract = {We propose a novel framework based on neural networks to identify the sentiment of opinion targets in a comment/review. Our framework adopts multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. The results of multiple attentions are non-linearly combined with a recurrent neural network, which strengthens the expressive power of our model for handling more complications. The weightedmemory mechanism not only helps us avoid the labor-intensive feature engineering work, but also provides a tailor-made memory for different opinion targets of a sentence. We examine the merit of our model on four datasets: two are from SemEval2014, i.e. reviews of restaurants and laptops; a twitter dataset, for testing its performance on social media data; and a Chinese news comment dataset, for testing its language sensitivity. The experimental results show that our model consistently outperforms the state-of-the-art methods on different types of data.},
 author = {Chen, Peng and Sun, Zhongqian and Bing, Lidong and Yang, Wei},
 title = {Recurrent attention network on memory for aspect sentiment analysis},
 pages = {452--461},
 booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
 year = {2017}
}


@inproceedings{chen2018,
 abstract = {Recently, deep learning approaches have been widely used in language modeling and achieved great success. However, the out-of-vocabulary (OOV) words are often estimated in a rather crude way using only one special symbol, which ignores the linguistic information. In this paper we present an LSTM language model with structured word embeddings to tackle this problem. In our model, both input and output embeddings of LSTM language model are deployed with structured word embeddings. Utilizing syntactic-level and morphological-level parameters sharing, OOV words can be incorporated into the proposed model without retraining. The LSTM language model with structured word embeddings is instantiated for Chinese. Experiments show that the proposed model achieves PPL improvement on OOV words, and can be further integrated into automatic speech recognition systems for fast vocabulary updating.},
 author = {Chen, R. and Yu, K.},
 title = {Fast Oov Words Incorporation Using Structured Word Embeddings for Neural Network Language Model},
 keywords = {OOV Literature},
 pages = {6119--6123},
 booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 year = {2018},
 doi = {10.1109/ICASSP.2018.8461491}
}


@inproceedings{collobert2008,
 author = {Collobert, Ronan and Weston, Jason},
 title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
 url = {http://doi.acm.org/10.1145/1390156.1390177},
 pages = {160--167},
 publisher = {ACM},
 isbn = {978-1-60558-205-4},
 series = {ICML '08},
 booktitle = {Proceedings of the 25th International Conference on Machine Learning},
 year = {2008},
 address = {New York, NY, USA},
 doi = {10.1145/1390156.1390177}
}


@article{collobert2011,
 author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
 year = {2011},
 title = {Natural language processing (almost) from scratch},
 pages = {2493--2537},
 volume = {12},
 number = {Aug},
 journal = {Journal of machine learning research}
}


@article{deerwester1990,
 author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
 year = {1990},
 title = {Indexing by latent semantic analysis},
 pages = {391--407},
 volume = {41},
 number = {6},
 journal = {Journal of the American society for information science}
}


@inproceedings{dehongma2017,
 abstract = {Aspect-level sentiment classification aims at identifying the sentiment polarity of specific target in its context. Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling thier contexts via generating target-specific representations. However, these studies always ignore the separate modeling of targets. In this paper, we argue that both targets and contexts deserve special treatment and need to be learned their own representations via interactive learning. Then, we propose the interactive attention networks (IAN) to interactively learn attentions in the contexts and targets, and generate the representations for targets and contexts separately. With this design, the IAN model can well represent a target and its collocative context, which is helpful to sentiment classification. Experimental results on SemEval 2014 Datasets demonstrate the effectiveness of our model.},
 author = {{Dehong Ma} and {Sujian Li} and {Xiaodong Zhang} and {Houfeng Wang}},
 title = {Interactive Attention Networks for Aspect-Level Sentiment Classification},
 pages = {4068--4074},
 booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on  Artificial Intelligence, IJCAI-17},
 year = {2017},
 doi = {10.24963/ijcai.2017/568}
}


@inproceedings{dong,
 abstract = {We propose Adaptive Recursive Neural Network (AdaRNN) for target-dependent Twitter sentiment classification. AdaRNN adaptively propagates the sentiments of words to target depending on the context and syntactic relationships between them. It consists of more than one composition functions, and we model the adaptive sentiment propagations as distributions over these composition functions. The experimental studies illustrate that AdaRNN improves the baseline methods. Furthermore, we introduce a manually annotated dataset for target-dependent Twitter sentiment analysis.},
 author = {Dong, Li and Wei, Furu and Tan, Chuanqi and Tang, Duyu and Zhou, Ming and Xu, Ke},
 title = {Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification},
 pages = {49--54},
 publisher = {{Association for Computational Linguistics}},
 editor = {Toutanova, Kristina and Wu, Hua},
 booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 year = {2014},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/v1/P14-2009}
}


@incollection{dossantos2014,
 abstract = {Sentiment analysis of short texts such as single sentences and Twitter messages is challenging because of the limited contextual information that they normally contain. Effectively solving this task requires strategies that combine the small text content with prior knowledge and use more than just bag-of-words. In this work we propose a new deep convolutional neural network that ex-ploits from character-to sentence-level information to perform sentiment analysis of short texts. We apply our approach for two corpora of two different domains: the Stanford Sentiment Tree-bank (SSTb), which contains sentences from movie reviews; and the Stanford Twitter Sentiment corpus (STS), which contains Twitter messages. For the SSTb corpus, our approach achieves state-of-the-art results for single sentence sentiment prediction in both binary positive/negative classification, with 85.7{\%} accuracy, and fine-grained classification, with 48.3{\%} accuracy. For the STS corpus, our approach achieves a sentiment prediction accuracy of 86.4{\%}.},
 author = {{dos Santos}, Cicero and Gatti, Maira},
 title = {Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts},
 url = {http://www.aclweb.org/anthology/C14-1008},
 pages = {69--78},
 publisher = {{Dublin City University and Association for Computational Linguistics}},
 editor = {{Junichi Tsujii} and {Jan Hajic}},
 booktitle = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
 year = {2014},
 address = {Dublin, Ireland}
}


@article{frege1892,
 author = {Frege, Gottlob},
 year = {1892},
 title = {On sense and reference, 1892},
 pages = {563--583},
 journal = {Readings in the Philosophy of Language}
}


@inproceedings{gallwitz1996,
 abstract = {In almost all applications of automatic speech recognition, especially in spontaneous speech tasks, the recognizer vocabulary cannot cover all occurring words. There is always a significant amount of out-of-vocabulary words even when the vocabulary size is very large. We present a new approach for the integration of out-of-vocabulary words into statistical language models. We use category information for all words in the training corpus to define a function that gives an approximation of the out-of-vocabulary word emission probability for each word category. This information is integrated into the language models. Although we use a simple acoustic model for out-of-vocabulary words, we achieve a 6{\%} reduction of word error rate on spontaneous speech data with about 5{\%} out-of-vocabulary rate.},
 author = {Gallwitz, F. and Noth, E. and Niemann, H.},
 title = {A category based approach for recognition of out-of-vocabulary words},
 keywords = {OOV Literature},
 pages = {228--231},
 publisher = {{Institute of Electrical and Electronics Engineers}},
 isbn = {0-7803-3555-4},
 editor = {Bunnell, H. Timothy. Ed and Idsardi, William},
 booktitle = {ICSLP 96},
 year = {1996},
 address = {New York},
 doi = {10.1109/ICSLP.1996.607083}
}


@article{hochreiter1997,
 abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
 author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
 year = {1997},
 title = {Long Short-Term Memory},
 pages = {1735--1780},
 volume = {9},
 number = {8},
 issn = {0899-7667},
 journal = {Neural Computation},
 doi = {10.1162/neco.1997.9.8.1735}
}


@article{jasonweston2014,
 abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
 author = {{Jason Weston} and {Sumit Chopra} and {Antoine Bordes}},
 year = {2014},
 title = {Memory Networks},
 url = {http://arxiv.org/abs/1410.3916},
 volume = {abs/1410.3916},
 journal = {CoRR}
}


@inproceedings{jiang2011,
 abstract = {Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-ofthe-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets). However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration. According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification.},
 author = {Jiang, Long and Yu, Mo and Zhou, Ming and Liu, Xiaohua and Zhao, Tiejun},
 title = {Target-dependent Twitter Sentiment Classification},
 url = {http://dl.acm.org/citation.cfm?id=2002472.2002492},
 pages = {151--160},
 publisher = {{Association for Computational Linguistics}},
 isbn = {978-1-932432-87-9},
 series = {HLT '11},
 booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
 year = {2011},
 address = {Stroudsburg, PA, USA}
}


@book{junichitsujii2014,
 year = {2014},
 title = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
 url = {http://www.aclweb.org/anthology/C14-1},
 address = {Dublin, Ireland},
 publisher = {{Dublin City University and Association for Computational Linguistics}},
 editor = {{Junichi Tsujii} and {Jan Hajic}}
}


@inproceedings{kim,
 abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
 author = {Kim, Yoon},
 title = {Convolutional Neural Networks for Sentence Classification},
 pages = {1746--1751},
 publisher = {{Association for Computational Linguistics}},
 editor = {{Alessandro Moschitti}, Qatar Computing Research Institute and {Bo Pang}, Google and {Walter Daelemans}, University of Antwerp},
 booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 year = {2014},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/v1/D14-1181}
}


@inproceedings{kiritchenko,
 abstract = {Reviews depict sentiments of customers towards various aspects of a product or service. Some of these aspects can be grouped into coarser aspect categories. SemEval-2014 had a shared task (Task 4) on aspect-level sentiment analysis, with over 30 teams participated. In this paper, we describe our submissions, which stood first in detecting aspect categories, first in detecting sentiment towards aspect categories, third in detecting aspect terms, and first and second in detecting sentiment towards aspect terms in the laptop and restaurant domains, respectively.},
 author = {Kiritchenko, Svetlana and Zhu, Xiaodan and Cherry, Colin and Mohammad, Saif},
 title = {NRC-Canada-2014: Detecting Aspects and Sentiment in Customer Reviews},
 pages = {437--442},
 publisher = {{Association for Computational Linguistics}},
 editor = {Nakov, Preslav and Zesch, Torsten},
 booktitle = {Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)},
 year = {2014},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/v1/S14-2076}
}


@article{liu2012,
 author = {Liu, Bing},
 year = {2012},
 title = {Sentiment analysis and opinion mining},
 pages = {1--167},
 volume = {5},
 number = {1},
 journal = {Synthesis lectures on human language technologies}
}


@inproceedings{mikolov2013,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
 title = {Distributed Representations of Words and Phrases and Their Compositionality},
 url = {http://dl.acm.org/citation.cfm?id=2999792.2999959},
 keywords = {Word Embeddings},
 pages = {3111--3119},
 publisher = {{Curran Associates Inc}},
 series = {NIPS'13},
 booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
 year = {2013},
 address = {USA}
}


@article{mikolov2013b,
 author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
 year = {2013},
 title = {Efficient Estimation of Word Representations in Vector Space},
 keywords = {Word Embeddings},
 volume = {abs/1301.3781},
 journal = {CoRR}
}


@proceedings{nakov,
 abstract = {Reviews depict sentiments of customers towards various aspects of a product or service. Some of these aspects can be grouped into coarser aspect categories. SemEval-2014 had a shared task (Task 4) on aspect-level sentiment analysis, with over 30 teams participated. In this paper, we describe our submissions, which stood first in detecting aspect categories, first in detecting sentiment towards aspect categories, third in detecting aspect terms, and first and second in detecting sentiment towards aspect terms in the laptop and restaurant domains, respectively.},
 year = {2014},
 title = {Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {Nakov, Preslav and Zesch, Torsten},
 doi = {10.3115/v1/S14-2}
}


@proceedings{nakovb,
 year = {2015},
 title = {Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {Nakov, Preslav and Zesch, Torsten and Cer, Daniel and Jurgens, David},
 doi = {10.18653/v1/S15-2}
}


@article{naptali2012,
 author = {NAPTALI, Welly and TSUCHIYA, Masatoshi and NAKAGAWA, Seiichi},
 year = {2012},
 title = {Class-Based N-Gram Language Model for New Words Using Out-of-Vocabulary to In-Vocabulary Similarity},
 keywords = {OOV Literature},
 pages = {2308--2317},
 volume = {E95.D},
 number = {9},
 issn = {0916-8532},
 journal = {IEICE Transactions on Information and Systems},
 doi = {10.1587/transinf.E95.D.2308}
}


@inproceedings{nguyen2015,
 author = {Nguyen, Thien Hai and Shirai, Kiyoaki},
 title = {Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis},
 pages = {2509--2514},
 booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
 year = {2015}
}


@inproceedings{pang2002,
 author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
 title = {Thumbs Up?: Sentiment Classification Using Machine Learning Techniques},
 url = {https://doi.org/10.3115/1118693.1118704},
 pages = {79--86},
 publisher = {{Association for Computational Linguistics}},
 series = {EMNLP '02},
 booktitle = {Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10},
 year = {2002},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/1118693.1118704}
}


@article{pang2008,
 author = {Pang, Bo and Lee, Lillian and others},
 year = {2008},
 title = {Opinion mining and sentiment analysis},
 pages = {1--135},
 volume = {2},
 number = {1--2},
 journal = {Foundations and Trends$\backslash$textregistered in Information Retrieval}
}


@inproceedings{pennington,
 author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
 title = {Glove: Global Vectors for Word Representation},
 keywords = {Word Embeddings},
 pages = {1532--1543},
 publisher = {{Association for Computational Linguistics}},
 editor = {{Alessandro Moschitti}, Qatar Computing Research Institute and {Bo Pang}, Google and {Walter Daelemans}, University of Antwerp},
 booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 year = {2014},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/v1/D14-1162}
}


@inproceedings{pontiki,
 author = {Pontiki, Maria and Galanis, Dimitris and Pavlopoulos, John and Papageorgiou, Harris and Androutsopoulos, Ion and Manandhar, Suresh},
 title = {SemEval-2014 Task 4: Aspect Based Sentiment Analysis},
 pages = {27--35},
 publisher = {{Association for Computational Linguistics}},
 editor = {Nakov, Preslav and Zesch, Torsten},
 booktitle = {Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)},
 year = {2014},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/v1/S14-2004}
}


@inproceedings{pontikib,
 abstract = {SemEval-2015 Task 12, a continuation of SemEval-2014 Task 4, aimed to foster research beyond sentence- or text-level sentiment classification towards Aspect Based Sentiment Analysis. The goal is to identify opinions expressed about specific entities (e.g., laptops) and their aspects (e.g., price). The task provided manually annotated reviews in three domains (restaurants, laptops and hotels), and a common evaluation procedure. It attracted 93 submissions from 16 teams.},
 author = {Pontiki, Maria and Galanis, Dimitris and Papageorgiou, Haris and Manandhar, Suresh and Androutsopoulos, Ion},
 title = {SemEval-2015 Task 12: Aspect Based Sentiment Analysis},
 pages = {486--495},
 publisher = {{Association for Computational Linguistics}},
 editor = {Nakov, Preslav and Zesch, Torsten and Cer, Daniel and Jurgens, David},
 booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)},
 year = {2015},
 address = {Stroudsburg, PA, USA},
 doi = {10.18653/v1/S15-2082}
}


@article{rumelhart1988,
 author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J. and others},
 year = {1988},
 title = {Learning representations by back-propagating errors},
 pages = {1},
 volume = {5},
 number = {3},
 journal = {Cognitive modeling}
}


@article{singh2016,
 author = {Singh, Mittul and Greenberg, Clayton and Oualil, Youssef and Klakow, Dietrich},
 year = {2016},
 title = {Sub-Word Similarity based Search for Embeddings: Inducing Rare-Word Embeddings for Word Similarity Tasks and Language Modelling},
 url = {http://www.aclweb.org/anthology/C16-1194},
 keywords = {OOV Literature},
 pages = {2061--2070},
 journal = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers}
}


@inproceedings{socher2011,
 author = {Socher, Richard and Pennington, Jeffrey and Huang, Eric H. and Ng, Andrew Y. and Manning, Christopher D.},
 title = {Semi-supervised Recursive Autoencoders for Predicting Sentiment Distributions},
 url = {http://dl.acm.org/citation.cfm?id=2145432.2145450},
 pages = {151--161},
 publisher = {{Association for Computational Linguistics}},
 isbn = {978-1-937284-11-4},
 series = {EMNLP '11},
 booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
 year = {2011},
 address = {Stroudsburg, PA, USA}
}


@inproceedings{socher2013,
 author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
 title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
 url = {http://www.aclweb.org/anthology/D13-1170},
 pages = {1631--1642},
 publisher = {{Association for Computational Linguistics}},
 booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
 year = {2013}
}


@proceedings{su,
 year = {2016},
 title = {Proceedings of the 2016 Conference on Empirical Methods in Natural  Language Processing},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
 doi = {10.18653/v1/D16-1}
}


@incollection{sukhbaatar2015,
 abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
 author = {Sukhbaatar, Sainbayar and szlam, arthur and Weston, Jason and Fergus, Rob},
 title = {End-To-End Memory Networks},
 url = {http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf},
 pages = {2440--2448},
 publisher = {{Curran Associates, Inc}},
 editor = {{C. Cortes} and {N. D. Lawrence} and {D. D. Lee} and {M. Sugiyama} and {R. Garnett}},
 booktitle = {Advances in Neural Information Processing Systems 28},
 year = {2015}
}


@inproceedings{tang,
 author = {Tang, Duyu and Wei, Furu and Yang, Nan and Zhou, Ming and Liu, Ting and Qin, Bing},
 title = {Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification},
 pages = {1555--1565},
 publisher = {{Association for Computational Linguistics}},
 editor = {Toutanova, Kristina and Wu, Hua},
 booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 year = {2014},
 address = {Stroudsburg, PA, USA},
 doi = {10.3115/v1/P14-1146}
}


@inproceedings{tang2016,
 abstract = {We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.},
 author = {Tang, Duyu and Qin, Bing and Liu, Ting},
 title = {Aspect Level Sentiment Classification with Deep Memory Network},
 pages = {214--224},
 publisher = {{Association for Computational Linguistics}},
 editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
 booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural  Language Processing},
 year = {2016},
 address = {Stroudsburg, PA, USA},
 doi = {10.18653/v1/D16-1021}
}


@inproceedings{tang2016b,
 abstract = {Target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence representation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.},
 author = {Tang, Duyu and Qin, Bing and Feng, Xiaocheng and Liu, Ting},
 title = {Effective LSTMs for Target-Dependent Sentiment Classification},
 url = {http://www.aclweb.org/anthology/C16-1311},
 pages = {3298--3307},
 publisher = {{The COLING 2016 Organizing Committee}},
 booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
 year = {2016}
}


@proceedings{toutanova,
 year = {2014},
 title = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 address = {Stroudsburg, PA, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {Toutanova, Kristina and Wu, Hua},
 doi = {10.3115/v1/P14-1}
}


@inproceedings{vo2015,
 abstract = {Target-dependent sentiment analysis on Twitter has attracted increasing research attention. Most previous work relies on syntax, such as automatic parse trees, which are subject to noise for informal text such as tweets. In this paper, we show that competitive results can be achieved without the use of syntax, by extracting a rich set of automatic features. In particular, we split a tweet into a left context and a right context according to a given target, using distributed word representations and neural pooling functions to extract features. Both sentiment-driven and standard embeddings are used, and a rich set of neural pooling functions are explored. Sentiment lexicons are used as an additional source of information for feature extraction. In standard evaluation, the conceptually simple method gives a 4.8{\%} absolute improvement over the state-of-the-art on three-way targeted sentiment classification, achieving the best reported results for this task.},
 author = {Vo, Duy-Tin and Zhang, Yue},
 title = {Target-dependent Twitter Sentiment Classification with Rich Automatic Features},
 url = {http://dl.acm.org/citation.cfm?id=2832415.2832437},
 pages = {1347--1353},
 publisher = {{AAAI Press}},
 isbn = {978-1-57735-738-4},
 series = {IJCAI'15},
 booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
 year = {2015}
}


@inproceedings{wagner2014,
 author = {Wagner, Joachim and Arora, Piyush and Cortes, Santiago and Barman, Utsab and Bogdanova, Dasha and Foster, Jennifer and Tounsi, Lamia},
 title = {Dcu: Aspect-based polarity classification for semeval task 4},
 pages = {223--229},
 booktitle = {Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014)},
 year = {2014}
}


@inproceedings{wang,
 abstract = {Aspect-level sentiment classification is a finegrained task in sentiment analysis. Since it provides more complete and in-depth results, aspect-level sentiment analysis has received much attention these years. In this paper, we reveal that the sentiment polarity of a sentence is not only determined by the content but is also highly related to the concerned aspect. For instance, ``The appetizers are ok, but the service is slow.'', for aspect taste, the polarity is positive while for service, the polarity is negative. Therefore, it is worthwhile to explore the connection between an aspect and the content of a sentence. To this end, we propose an Attention-based Long Short-Term Memory Network for aspect-level sentiment classification. The attention mechanism can concentrate on different parts of a sentence when different aspects are taken as input. We experiment on the SemEval 2014 dataset and results show that our model achieves state-ofthe-art performance on aspect-level sentiment classification.},
 author = {Wang, Yequan and Huang, Minlie and zhu, xiaoyan and Zhao, Li},
 title = {Attention-based LSTM for Aspect-level Sentiment Classification},
 pages = {606--615},
 publisher = {{Association for Computational Linguistics}},
 editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
 booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural  Language Processing},
 year = {2016},
 address = {Stroudsburg, PA, USA},
 doi = {10.18653/v1/D16-1058}
}


@inproceedings{yang2016,
 abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
 author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
 title = {Hierarchical attention networks for document classification},
 pages = {1480--1489},
 booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 year = {2016}
}


@inproceedings{zhang2016,
 abstract = {Targeted sentiment analysis classifies the sentiment polarity towards each target entity mention in given text documents. Seminal methods extract manual discrete features from automatic syntactic parse trees in order to capture semantic information of the enclosing sentence with respect to a target entity mention. Recently, it has been shown that competitive accuracies can be achieved without using syntactic parsers, which can be highly inaccurate on noisy text such as tweets. This is achieved by applying distributed word representations and rich neural pooling functions over a simple and intuitive segmentation of tweets according to target entity mentions. In this paper, we extend this idea by proposing a sentencelevel neural model to address the limitation of pooling functions, which do not explicitly model tweet-level semantics. First, a bi-directional gated neural network is used to connect the words in a tweet so that pooling functions can be applied over the hidden layer instead of words for better representing the target and its contexts. Second, a three-way gated neural network structure is used to model the interaction between the target mention and its surrounding contexts. Experiments show that our proposed model gives significantly higher accuracies compared to the current best method for targeted sentiment analysis.},
 author = {Zhang, Meishan and Zhang, Yue and Vo, Duy-Tin},
 title = {Gated Neural Networks for Targeted Sentiment Analysis},
 url = {http://dl.acm.org/citation.cfm?id=3016100.3016334},
 pages = {3087--3093},
 publisher = {{AAAI Press}},
 series = {AAAI'16},
 booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
 year = {2016}
}


@article{zheng2018,
 abstract = {Deep learning techniques have achieved success in aspect-based sentiment analysis in recent years. However, there are two important issues that still remain to be further studied, i.e., 1) how to efficiently represent the target especially when the target contains multiple words; 2) how to utilize the interaction between target and left/right contexts to capture the most important words in them. In this paper, we propose an approach, called left-centerright separated neural network with rotatory attention (LCR-Rot), to better address the two problems. Our approach has two characteristics: 1) it has three separated LSTMs, i.e., left, center and right LSTMs, corresponding to three parts of a review (left context, target phrase and right context); 2) it has a rotatory attention mechanism which models the relation between target and left/right contexts. The target2context attention is used to capture the most indicative sentiment words in left/right contexts. Subsequently, the context2target attention is used to capture the most important word in the target. This leads to a two-side representation of the target: left-aware target and right-aware target. We compare our approach on three benchmark datasets with ten related methods proposed recently. The results show that our approach significantly outperforms the state-of-the-art techniques.},
 author = {Zheng, Shiliang and Xia, Rui},
 year = {2018},
 title = {Left-Center-Right Separated Neural Network for Aspect-based Sentiment Analysis with Rotatory Attention},
 journal = {arXiv preprint arXiv:1802.00892}
}


