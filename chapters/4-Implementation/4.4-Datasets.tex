% !TEX root = ../../fyp.tex
\documentclass[../../fyp.tex]{subfiles}

\begin{document}
The following section presents a brief overview of the datasets that were chosen to be integrated into the framework. We focus on the principal factors that were considered when choosing these datasets with the intention of maximizing the diversity of these attributes. These include the mechanism by which the datasets were originally obtained and annotated, the respective platform from where they were sourced, any specific domain around which they center and finally we present any salient statistical information that characterizes the datasets.

\subsection{Dong Twitter Dataset} \label{ds:dong}
\citet{dong} is one of the most frequently cited datasets for TSA, used in works such as \citep*{tang2016b,chen2017,zheng2018}. This dataset comprises tweets focusing on a diverse range of topics that were obtained through the twitter streaming API by querying for targets such as \enquote{bill gates} and \enquote{xbox}. This implies that, in each instance, the target string occurs explicitly in the tweet. The respective sentiment of each tweet towards its target is manually annotated as \enquote{positive}, \enquote{neutral} or \enquote{negative}, and the authors report a fair inter-annotator agreement score of $0.82$.  The final dataset consists of 6248 training samples and 692 testing samples, each having a distribution consisting of twice as many neutral samples as positive and negative combined. As some studies \citep*{wang2017,saeidi2016} that followed point out, a notable property of this dataset is that each sample is limited to a single target. It is also worth noting that due to the skewed nature of the class distribution, it is imperative that a metric that accounts for this is reported, such as the Macro-F1, when evaluating performance on this dataset.

\subsection{Saeidi Yahoo Answers Dataset} \label{ds:saeidi}
In their work, \citet{saeidi2016} combine the tasks of detecting the existence of a particular aspect as well as the sentiment with respect to that aspect within pieces of text obtained from \enquote{Yahoo! Answers} concerning specific neighborhoods in or around the city of London. The authors suggest that this text tends to be less constrained than that originating from social media platforms, and that this enables them to obtain reviews that mention multiple neighborhoods per instance with differing sentiments. The final dataset is limited to samples that contain up to two targets and these are manually annotated using binary labels (\enquote{positive} and \enquote{negative}) across eleven separate aspects such as \enquote{dining} and \enquote{shopping}. These aspects do not necessarily occur within the text itself and are therefore inconsequential within the scope of TSA. This notwithstanding, a \enquote{general} aspect category is also considered, used to denote a \textit{general} sentiment towards a particular entity or target, and is the most frequently annotated category. We consider only this subset of samples from this dataset and only use the train and test portions, discarding the development portion. It is important to note that all original target mentions within this dataset are obfuscated through the use of \enquote{location markers} which eliminates any potential context information conveyed by the original target. The final datasets consists of 1770 sentences, 78\% of which are labelled positive. 

\subsection{Wang Political Twitter Dataset} \label{ds:wang} 
A point that Saeidi \cite{saeidi2016} stresses on in developing their dataset is the importance of considering an approach's ability to discern different and possibly opposing sentences towards multiple targets in the same phrase. With this same goal in mind, \citet{wang2017} build a dataset consisting of tweets centered around UK politics, labelled according to the sentiment expressed towards a specific target entity that occurs in the tweet, using a 3-point scale. Across 4077 tweets, a total of 12587 targets are identified with just under half (6015) of those expressing a negative sentiment and an average of 3.09 targets per sentence. As pointed out in \citet{moore2018}, there are a number of samples in this dataset where the target locations are not specified, which makes tweets where the target occurs more than once unusable since there is no way of knowing which instance is being considered. We exclude these cases for our experiments, resulting in a reduced dataset which contains 11899 target-sentiment pairs in total; with 2190 unique targets and an average of 2.94 targets per tweet.

%TODO mention how prevalent the conflict label is and what we do with them in our work
\subsection{SemEval Laptop \& Restaurant Reviews Dataset} \label{ds:pontiki}
Another dataset commonly cited in the field of targeted sentiment analysis and aspect-based sentiment analysis is the Restaurant and Laptop Reviews dataset \cite{pontiki}. This dataset comprises 7686 sentences from two distinct domains, technology and dining, from which specific aspect terms and the polarity towards them were manually annotated. Sentiment polarity was determined on a 3 point scale with the addition of a \enquote{conflict} label. Similar to \citet{dehongma2017}, as well as \citet{chen2017} and \citet{tang2016b}, we discard \enquote{conflict} samples for the purpose of our experiments, which amount to 61(2.07\%) and 105(2.22\%) samples for the laptops and restaurants datasets, respectively. Compared to data sourced from micro-text platforms the samples in this dataset are typically lengthier and more syntactically sound. The authors note some interesting differences in the nature of the reviews obtained from the two domains such as the restaurant subset containing substantially more aspect terms and a stronger bias towards the positive sentiment compared to the laptop reviews. The majority of targets in both domains consist of a single word. Recently, \citet{xue2018} used this dataset for their evaluation purposed and included results for three of the five baselines implemented in our work, namely, IAN \cite{dehongma2017}, RAM \cite{chen2017} and TD-LSTM \cite{tang2016b}.

\subsection{SemEval 2015 Twitter Dataset} \label{ds:resenthal}
This dataset was specifically proposed for the task of sentiment analysis on twitter as part of the SemEval challenge \cite{rosenthal2015}. A subset of the dataset which we are interested in within the confines of this study consists of a series of tweets which were collected and manually annotated with a 3-scale sentiment polarity score directed towards a particular entity term occurring in the tweet. The annotation process was carried out using Amazon's Mechanical Turk system with 5 turks, employing a majority vote in cases where an agreement could not be obtained. For the purposes of our study we make use of the training and testing subsets of the dataset and interchange these two subsets so as to have a larger training dataset as opposed to testing set, as in the previous datasets outlined. Moreover, in its original form this dataset consisted only of tweet IDs  from this the original tweet can be obtained however only within a specific window. In our experiments we were able to obtain a  downloaded version of this data, which included the original tweet text, from a later SemEval submission \cite{baziotis2017} from their github repository. The final dataset consists of 2872 samples, the majority of which are labelled \enquote{positive}.

\subsection{SemEval 2016 Twitter Dataset} \label{ds:nakov}
Similar to the previously mentioned dataset, this was also constructed for the purposes of the SemEval sentiment analysis on twitter challenge \cite{nakov2016}, with the difference of being graded on a 5-point scale as opposed to a 3-point and also comprising substantially more samples and topics than the former. For the purposes of our work, we reduce the resolution of this dataset from a 5-point scale to a 3-point scale by grouping \enquote{very negative} and \enquote{weakly negative} samples under a single \enquote{negative} label, and similarly for \enquote{very positive} and \enquote{weakly positive}, to obtain \enquote{positive} samples. The annotation process that was employed is similar to that outlined by \citet{rosenthal2015} with minor differences in the majority voting system for conflict resolution around label disagreements due to the different point-scale (the process is detailed in \cite{nakov2016}). The actual tweet data for this dataset was also obtained from the Github repository of \cite{baziotis2017} due to the same limitation around this data expiring after a particular time-window. The test and train splits of the dataset were similarly interchanged to obtain a larger training dataset. The final dataset includes over 20000 training samples covering 100 topics, with the overwhelming majority being either \enquote{neutral} or \enquote{positive}. Whereas the testing dataset consists of 6000 samples with entirely different topics and is skewed towards the \enquote{positive} label.

\begin{table}
	\centering
	\begin{tabular}{|l|l|l|l|c|c|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Dataset}} & \multicolumn{1}{c|}{\textbf{Domain}} & \multicolumn{1}{c|}{\textbf{Type}} & \multicolumn{1}{c|}{\textbf{Split}} & \textbf{\begin{tabular}[c]{@{}c@{}}Vocabulary \\ Size\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Unique \\ Targets\end{tabular}} \\ \hline\hline
		\multirow{3}{*}{Dong}                  & \multirow{3}{*}{General}             & \multirow{3}{*}{Social Media}      & Test                                & 3682                               & 82                                 \\
		                                       &                                      &                                    & Train                               & 15037                              & 113                                \\ \cline{4-6}
		                                       &                                      &                                    & \textbf{Total}                      & \textbf{16047}                     & \textbf{118}                       \\ \hline\hline
		\multirow{3}{*}{Saeidi}                & \multirow{3}{*}{General}             & \multirow{3}{*}{Review}            & Test                                & 1231                               & 2                                  \\
		                                       &                                      &                                    & Train                               & 1804                               & 2                                  \\ \cline{4-6}
		                                       &                                      &                                    & \textbf{Total}                      & \textbf{2257}                      & \textbf{2}                         \\ \hline\hline
		\multirow{3}{*}{Wang}                  & \multirow{3}{*}{Politics}            & \multirow{3}{*}{Social Media}      & Test                                & 4385                               & 755                                \\
		                                       &                                      &                                    & Train                               & 9706                               & 1855                               \\ \cline{4-6}
		                                       &                                      &                                    & \textbf{Total}                      & \textbf{11211}                     & \textbf{2190}                      \\ \hline\hline
		\multirow{3}{*}{Pontiki - R}           & \multirow{3}{*}{Restaurants}         & \multirow{3}{*}{Review}            & Test                                & 2199                               & 553                                \\
		                                       &                                      &                                    & Train                               & 4309                               & 1268                               \\ \cline{4-6}
		                                       &                                      &                                    & \textbf{Total}                      & \textbf{5122}                      & \textbf{1630}                      \\ \hline\hline
		\multirow{3}{*}{Pontiki - L}           & \multirow{3}{*}{Technology}          & \multirow{3}{*}{Review}            & Test                                & 1573                               & 415                                \\
		                                       &                                      &                                    & Train                               & 3638                               & 1031                                \\ \cline{4-6}
		                                       &                                      &                                    & \textbf{Total}                      & \textbf{4090}                      & \textbf{1295}                      \\ \hline\hline
		\multirow{3}{*}{Rosenthal}             & \multirow{3}{*}{General}             & \multirow{3}{*}{Social Media}      & Test                                & 3243                               & 44                                 \\
		                                       &                                      &                                    & Train                               & 9800                               & 137                                \\ \cline{4-6}
		                                       &                                      &                                    & \textbf{Total}                      & \textbf{11575}                     & \textbf{180}                       \\ \hline\hline
		\multirow{3}{*}{Nakov}                 & \multirow{3}{*}{General}             & \multirow{3}{*}{Social Media}      & Test                                & 19374                              & 60                                 \\
		                                       &                                      &                                    & Train                               & 43859                              & 100                                \\ \cline{4-6}
		                                       &                                      &                                    & \textbf{Total}                      & \textbf{54034}                     & \textbf{160}                       \\ \hline
	\end{tabular}
	\caption{Dataset domains, vocabulary sizes, and the number of unique targets.}
	\label{tab:dataset_domain_vocab}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{|l|ccc|}
		\hline
		\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Dataset}}} & \multicolumn{3}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Target Length\\ (tokens)\end{tabular}}}                            \\ \cline{2-4}
		\multicolumn{1}{|c|}{}                                  & \textbf{1}                                                   & \textbf{2} & \textbf{3+} \\ \hline
		Dong                                                    & 2080                                                         & 4851       & 9           \\
		Saeidi                                                  & 1770                                                         & 0          & 0           \\
		Wang                                                    & 9702                                                         & 1937       & 260         \\
		Pontiki - R                                             & 3504                                                         & 791        & 427         \\
		Pontiki - L                                             & 1804                                                         & 824        & 323         \\
		Rosenthal                                               & 1554                                                         & 1145       & 170         \\
		Nakov                                                   & 11450                                                        & 13684      & 1366        \\ \hline
	\end{tabular}
	\caption{Number of targets with different lengths, in tokens.}
	\label{tab:dataset_target_length}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{|l|l|c|ccc|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Dataset}} & \multicolumn{1}{c|}{\textbf{Split}} & \textbf{Samples} & \textbf{Negative} & \textbf{Neutral} & \textbf{Positive} \\ \hline\hline
		\multirow{3}{*}{Dong}                  & Test                                & 692              & 173               & 346              & 173               \\
		                                       & Train                               & 6248             & 1560              & 3127             & 1561              \\ \cline{2-6}
		                                       & \textbf{Total}                      & \textbf{6940}    & \textbf{1733}     & \textbf{3473}    & \textbf{1734}     \\ \hline\hline
		\multirow{3}{*}{Saeidi}                & Test                                & 588              & 139               & -                & 449               \\
		                                       & Train                               & 1182             & 246               & -                & 936               \\ \cline{2-6}
		                                       & \textbf{Total}                      & \textbf{1770}    & \textbf{385}      & \textbf{-}       & \textbf{1385}     \\ \hline\hline
		\multirow{3}{*}{Wang}                  & Test                                & 2541             & 1206              & 957              & 378               \\
		                                       & Train                               & 9358             & 4377              & 3615             & 1366              \\ \cline{2-6}
		                                       & \textbf{Total}                      & \textbf{11899}   & \textbf{5583}     & \textbf{4572}    & \textbf{1744}     \\ \hline\hline
		\multirow{3}{*}{Pontiki - R}           & Test                                & 1120             & 196               & 196              & 728               \\
		                                       & Train                               & 3602             & 805               & 633              & 2163              \\ \cline{2-6}
		                                       & \textbf{Total}                      & \textbf{4722}    & \textbf{1001}     & \textbf{829}     & \textbf{2891}     \\ \hline\hline
		\multirow{3}{*}{Pontiki - L}           & Test                                & 638              & 128               & 169              & 341               \\
		                                       & Train                               & 2313             & 866               & 460              & 987              \\ \cline{2-6}
		                                       & \textbf{Total}                      & \textbf{2951}    & \textbf{994}      & \textbf{629}     & \textbf{1328}     \\ \hline\hline
		\multirow{3}{*}{Rosenthal}             & Test                                & 486              & 56                & 288              & 142               \\
		                                       & Train                               & 2383             & 260               & 1256             & 867               \\ \cline{2-6}
		                                       & \textbf{Total}                      & \textbf{2869}    & \textbf{316}      & \textbf{1544}    & \textbf{1009}     \\ \hline\hline
		\multirow{3}{*}{Nakov}                 & Test                                & 5868             & 749               & 1623             & 3496              \\
		                                       & Train                               & 20632            & 2339              & 10081            & 8212              \\ \cline{2-6}
		                                       & \textbf{Total}                      & \textbf{26500}   & \textbf{3088}     & \textbf{11704}   & \textbf{11708}    \\ \hline
	\end{tabular}
	\caption{Dataset sizes and class distributions. }
	\label{tab:dataset_class_dists}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{|l|c|cc|ccc|}
		\hline
		\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Average Targets\\ /Sentence\end{tabular}}} & \multicolumn{2}{l|}{\textbf{\begin{tabular}[c]{@{}c@{}}Sentence Length\\ (tokens)\end{tabular}}} & \multicolumn{3}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Distinct Sentiments\\ /Sentence\end{tabular}}}                                         \\ \cline{3-7}
		                                  &                                                          & \textbf{Average}                                             & \textbf{/Target}                                             & \textbf{1} & \textbf{2} & \textbf{3+} \\ \hline
		Dong                              & 1                                                        & 21.1                                                         & 21.1                                                         & 6934       & 0          & 0           \\
		Saeidi                            & 1.24                                                     & 15.2                                                         & 12.4                                                         & 1416       & 23         & 0           \\
		Wang                              & 2.94                                                     & 26.3                                                         & 9                                                            & 2163       & 1640       & 242         \\
		Pontiki - R                       &  1.83                                                    & 20                                                           & 9.4                                                          & 2177       & 379        & 20          \\
		Pontiki - L                       & 1.58                                                     & 21.4                                                         & 12                                                           & 1665       & 193        & 9           \\
		Rosenthal                         & 1.01                                                     & 23                                                           & 22.8                                                         & 2842       & 5          & 0           \\
		Nakov                             & 1.01                                                     & 23.2                                                         & 23.1                                                         & 26355      & 40         & 0           \\ \hline
	\end{tabular}
	\caption{Dataset sentence information}
	\label{tab:dataset_sentence_info}
\end{table}
\end{document}