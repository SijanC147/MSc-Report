% !TEX root = ../../fyp.tex
\documentclass[../../fyp.tex]{subfiles}

\begin{document}
At a high level, all operations that take place in the experiment module are governed by the experiment class, which simultaneously serves the purpose of maintaining an indexed directory of experiments while also preparing and dispatching commands to run those experiments. In this context, preparing an experiment implies setting up the directory for a new experiment, bridging the feature provider and model being trained and, loading any environment configuration settings for TensorFlow and scaffolding the experiment remotely on \textit{comet.ml}. Aside from the this tooling options that \textit{comet.ml} provides, the primary motivating factor behind integrating it into the framework is to facilitate collaborative work by making experiment details and results remotely accessible, speeding up development by automatically setting up the requisite code infrastructure.

The experiment class automatically organizes the experiment directory first by model name, and then either by the specific name for an experiment which is provided by the user, or using a name automatically generated from the experiment parameters provided. Enforcing this approach has two benefits, first, experiment directories are self-documenting since they convey the details of the experiment, secondly, the framework will use the directory and continue training for an existing experiment with the same parameters by default.

Finally, since each experiment also serves as a self-contained model, the experiment class is also responsible for exporting trained models from an experiment. Once exported, trained models can be evaluated using real-world data. Using this data as a means of ensuring the validity of the process as a whole was the motivation behind implementing this functionality.

\subsection{TSA Model Base Class}
The Targeted-Sentiment-Analysis (TSA) Model base class serves as the abstraction layer between the end-user and the internal infrastructure of the framework. The implementation adheres to the principle of least privilege, exposing only the core constructs of a TSA model while handling the integration of that model with the rest of the frameworks' features internally. The core constructs for TSA model development are the default parameters of the model and the tensor operations it performs. A third, optional construct, covers any feature pre-processing where feature mutations can be specified that take place before invoking the model definition function. Exposing only these core constructs greatly simplifies development, enabling the user to focus on the model being developed. Moreover, this ensures that the rest of the framework components, particularly those concerning evaluation remain pristine and consistent across different model implementations. 

The parameter definition function is the simplest of the three, only required to return a dictionary of default parameter values which may be overridden at runtime before being made available within the scope of the model definition function.

The default feature set is a dictionary of literal string tokens and their respective embedding IDs for target and their contexts (left and right seprately). An optional pre-processing function can be specified to mutate the samples to prior to consumption by the model function so as to obtain the features desired by the end-user. This design choice was made to decouple the pre-processing logic from the model definition, making it possible to inspect and test the process in isolation. Typical pre-processing operations may include concatenating the target to the left and right contexts. 

The feature pre-processing function must return a dictionary of features, which the framework internally re-couples with the label data. For this reason, the pre-processing function only may only mutate the structure each sample, mainting the length and order of the features as a whole, otherwise this could result in features being assigned incorrect labels. Token string data is required for certain downstream visualizations, such as attention heatmaps, where token IDs are not sufficiently informative. It is worth noting that the string literals are not automatically mutated, and maintaining integrity between IDs and string literals must be handled by the end-user. 

Finally, before exposing these features to the model definition function, the framework automatically appends \texttt{*\_emd} and \texttt{*\_len} entries for each \texttt{*\_ids} key on the dictionary, representing the embedded sequences and their length, respectively.

The model definition function defines the tensor operations to carry out on the feature data, producing logits for each sample. It exposes four parameters within its scope: a dictionary of features, their corresponding labels, a flag denoting whether the model is being trained, evaluated or running as a prediction service, and finally, a dictionary containing the resolved set of model parameters. Using the produced logits in conjunction with a loss function and optimization technique, which are also defined in this scope, the function must return a call to an in-built class method that generates the \enquote{estimator spec} object which encapsulates the model's behavior. 

Adhering to the design specifications described herein, the framework is able to internally plug into the TSA model and handle the necessary code infrastructure adding functionality such as online collaboration, various downstream evaluation visualizations and introspection tools, automatic embedding of sequence data, support for Google cloud services and finally, a streamlined process for exporting the trained model as a prediction service. 

Certain features, specifically those centering around evaluation methods, such as attention heat-maps, are only applicable to a subset of model types. To address these situations efficiently an add-on mechanism that could isolate particular functionalities and automatically make them accessible to all TSA models was developed. 

\subsection{Model Add-On API}
The principal motivation behind the add-on API is to enforce a design pattern where a TSA model implementation consists of code concerned solely with the definition of the model. Moreover, due to the importance of visualizations for evaluation purposes, it became apparent that this process would need to be streamlined to meet our objective of facilitating rapid experimentation. The add-on mechanism achieves this by allowing code concerned with specific functionality to be isolated, preventing repetitive code, reducing development time and facilitating testing. Furthermore, while the TSA model class provides out-of-the-box functionality, in some aspects it could be considered a black-box approach, limiting the end user. The add-on API aims to address this by providing an entry point for customization at a deeper level while still maintaining a layer of abstraction between the user and the framework's internals.

Each add-on functions as a pass-through for the estimator spec object generated by the model definition function, enabling the user to inject code before running the model. These commonly take the form of wrappers around hooks, a native Tensorflow construct. Hooks enable the execution of custom code at different points in the model lifecycle such as specific steps during training or evaluation. The applications for these hooks include producing custom visualizations, logging additional metrics and enabling techniques such as early stopping, to name a few. Within the scope of an add-on function, the user has access to all of the same variables available in the model definition function, including the model instance itself, as well as the incoming estimator spec object, which must be returned, so as to either be passed on to the next add-on or execute the model. There are no requirements on the internals of the add-on or any custom hooks that the add-on wraps around, however, it must conform to a function signature to integrate with the framework. A number of examples of this signature are provided in the \texttt{addons.py} file along with custom hooks implemented specifically for the purpose of our work in the \texttt{hooks.py} file.  

An addon decorator is provided which takes as input an array of one or more add-ons to attach to the model definition function it \textit{decorates}. Add-ons run in the order they are attached, after the model definition function is called and an initial estimator spec object has been produced. Additionally, each add-on can be instructed to run only in a subset of scenarios, specifically: training, evaluation, and prediction. Finally, each add-on can be deactivated at runtime through when invoking a new task using the CLI. This is useful when submitting batch jobs to cloud service where there is no access to the codebase in between tasks and add-ons can only be deactivated programmatically between tasks.
\end{document}