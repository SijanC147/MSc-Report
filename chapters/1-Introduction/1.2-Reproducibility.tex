% !TEX root = ../../fyp.tex
\documentclass[../../fyp.tex]{subfiles}

\begin{document}
The ability to reproduce experiments is the integral basis upon which all disciplines of science are founded. Within many fields, NLP among them, this typically entails adherence to three integral guidelines, namely, (a) the provision of sufficiently detailed methodologies, (b) the release of operational code-bases and (c) access to the dataset(s) with clear details pertaining to any processing and/or stratification strategies used. These guidelines ensure that results can be easily reproduced, evaluated for generalizability and compared to other methods in the field, thereby fostering growth.

% \subsection{Reproducibility in TSA}
\citet{moore2018} underscore the significance of reproducibility of approaches as well as the generalizability of the results that are reported, and proceed to argue that adherence to the aforementioned tenets has been lacking in recent years, paying particular attention in their work to the field of targeted sentiment analysis.

The authors also draw attention to the fact that a multitude of studies report results on different datasets which often stem from diverse sources that could be composed of language that is centered around a particular topic. Notable still, these datasets also exhibit consequential statistical differences such as the average length of, and/or amount of targets in, a sentence. Occasionally, studies also carry out particular alterations to existing datasets or adopt a specific strategy for merging one or more datasets (eg. \citet{xue2018}). These factors substantially limit the possibility of effectively comparing the novel approaches as they emerge in the field.

Replication studies such as \citep{moore2018} can remedy this issue by attempting to reproduce the studies in a comparative setting, however they outline challenges in this regard as well. The authors note that a number of approaches in the field fail to outline the precise pre-processing steps they adopted, which may have substantial effects on the downstream performance of a model, and thus make reproducing the results difficult. In some situations they also note model settings mentioned that are not necessarily self-evident, such as a \enquote{softmax clipping threshold} \citep{tang2016b}, which, in attempting to reproduce the study, \citet{moore2018} were forced to ignore as they were unfamiliar with the term.

One key observation that \citet{moore2018} also make with regards to deep learning and neural network based approaches such as \citet{tang2016b} is the influence that an initial random seed has on the final performance results, particularly when using smaller word embeddings (\citet{reimers2017} as cited in \citet{moore2018}). They mention this as the probable reason for other studies \citep{chen2017,tay2017} not being able to recreate the original results reported in \citet{tang2016b}, including their own. The authors remark that in situations such as this, when dealing with models of this nature, it would be well-advised to gauge a model's performance over a number of experiment runs as opposed to one.

Moore's work sheds light on the most serious obstacles to reproducibility in the field of TSA which make determining the current state-of-the-art unnecessarily more challenging and time consuming. We believe this problem can be remedied using a holistic approach, encompassing all stages of a machine learning TSA model pipeline from data processing to results' evaluation in a framework that makes sure the design process adheres to the guidelines in \citep{moore2018} at every stage. Moreover, by abstracting these considerations from the end-user and a TSA model specification, these features can be shared across TSA model implementations, saving time and providing a consistent environment in which TSA model specifications can be compared, all other things being equal. 

\end{document}