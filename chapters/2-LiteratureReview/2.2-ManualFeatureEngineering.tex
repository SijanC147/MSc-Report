% !TEX root = ../../fyp.tex
\documentclass[../../fyp.tex]{subfiles}

\begin{document}
Initially, the conventional approach involved manually extracting the most expressive and information rich features from sentences that would subsequently be processed through some statistical model such as a Support Vector Machine (SVM) for classification.

This entailed the formulation of processes by which to obtain these features, and was normally preceded by some form of normalization of the original data before these features could be extracted. Typically many types of these features were used in conjunction, each intended to extrapolate differing particularities about a specific aspect of the text, such as whether a specific token represented a noun or an adjective, or details about the words surrounding it.

The capacity of the SVM had been demonstrated on the general task of sentiment analysis in works such as \cite{pang2002}, as well as other tools such as, bag-of-words, part-of-speech tags and other morphological and syntactical features and external resources such as linguistic parsers and sentiment lexicon, employed in works such as \cite{dong}, \cite{vo2015}, \cite{nguyen2015}.

However, as \cite{tang2016b} point out, these methods would implicitly impose an external dependency on the system. Moreover, within the context of social media, where conventional rules of language are often times regarded rather as guidelines, various studies question the applicability of dependency parsing techniques that rely on a particular degree of formality, or structure, within the content itself \cite{tang2016b}, \cite{chen2017}. Nevertheless these features have proven their worth when used in conjunction with powerful models such as the aforementioned SVM \cite{kiritchenko} \cite{wagner2014}, as well as neural networks \cite{dong}, \cite{vo2015}, in predicting sentiment polarity.

Even in the work that followed, focusing on increasingly autonomous feature extraction methods and more sophisticated deep learning architectures such as the Long Short Term Memory (LSTM) model, \cite{tang2016b} make note of the competitive results obtained by the SVM approach in \cite{kiritchenko} when compared to their implementations.

\subsection{Drawbacks}
Although works such as \cite{kiritchenko}, \cite{wagner2014}) obtained encouraging results, much of the subsequent literature recognizes that these results were exceedingly contingent on the choice of features \cite{tang2016b}.

Although the manual feature-based approach fared well in their work, \cite{tang2016b} suggest that features of this kind lack the required resolution of detail that would accurately capture the interplay between target and context. The features that had been used had sound rationales behind them, however devising these rationales was in itself becoming increasingly time-consuming. One reason for this is scalability; with the increase of data available, more considerations and specifics must be accounted for when manually devising these features.

As alluded to by \cite{zheng2018}, with the aforementioned increase in labor involved, these approaches could be regarded as a bottleneck in terms of performance of these models and the wealth of data available. A more autonomous solution that would accurately capture the intricacies of language from an expansive wealth of text at a deeper level, not contingent on a proportionally large amount of labor-intensive manual feature-engineering, was desired to further advance TSA.
\end{document}