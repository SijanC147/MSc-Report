% !TEX root = ../../fyp.tex
\documentclass[../../fyp.tex]{subfiles}

\begin{document} 
\subsection{Challenges}
As with any task that requires a deeper understanding of the intricacies of language, there are many challenges that face TSA. Many of these challenges are inherent to parsing the structure of a language such as sarcasm, where sentences such as \enquote{Nice perfume. You must shower in it.} \cite{kharde2016} are composed entirely of positive-sentiment bearing words while expressing a negative sentiment. These notwithstanding, the informal nature of the majority of text data found on social media platforms (upon which this task frequently focuses), supplements these challenges with its own.

Colloquialisms and social short-hands are commonplace within social media networks where many users intend to convey as much information in as few characters as possible, particularly in situations where this number is capped. This phenomenon also leads to intentional, as well as unintentional, spelling errors which further obscures that data for any prospective machine learning model that does not account for these circumstances.

Along with these challenges, the literature also presents a number of obstacles and particularly problematic instances that need to be taken into account when approaching the task of targeted sentiment analysis. Comparative opinions are one such circumstance where the sentiment being conveyed is obscured by another subject. Tang et al. \cite{tang2016} report challenges of this sort, with phrases such as \enquote{I've had better Japanese food at a mall food court}.

Other common challenges that are pointed out in \cite{tang2016}, are negation and conditional situations, citing the example \enquote{but dinner here is never disappointing, even if the prices are a bit over the top}, where the sentiment towards the target cannot be easily deduced from the various syntactic structures present.

Moreover, the particular case of expressions which consist of multiple words needs to be given special care. Various approaches that employ word embeddings operate on the word as the atomic unit of operation, and would therefore struggle to correctly model an expression such as \enquote{die for} in \enquote{the ice cream to die for} \cite{tang2016} from its constituents. Zheng et al. \cite{zheng2018} also also stress the significance of this issue, and argue that it has not been given sufficient attention, particularly when modelling \textit{targets} that also consist of multiple words.

When considering the opinion of a sentence towards a specific target, it may be the case that the sentence will have opposing sentiments for different targets, this is another degree of complexity that targeted-sentiment analysis models need to account for as opposed to sentiment analysis of the sentence as a whole \cite{tang}. Phrases such as \enquote{great food but the service was dreadful!} convey different and opposite sentiments towards \enquote{food} and \enquote{service} \cite{tang2016}. Previous sentence oriented sentiment analysis approaches such as \cite{socher2011}, \cite{appel2016} would be incapable of correctly distinguishing this level of granularity \cite{chen2017}.

Ma et al. \cite{dehongma2017}, Wang et al. \cite{wang2018} also call attention to the fact that there are several instances where the sentiment conveyed by a particular word is contingent upon the target or aspect that is being considered. An adjective such as \enquote{short} can have positive connotations with respect to \enquote{waiting times} for a restaurant, on the other hand the same adjective is assumed negative when describing something such as the \enquote{battery life} of a product.

\subsection{Common Evaluation Metrics}\label{sec:eval_metrics}
\begin{table}[h!]
	\centering
	\begin{tabular}{||l c c||}
		\hline
		                 & $y=A$                 & $y\neq{A}$            \\ [0.5ex]
		$\hat{y}=A$      & true positive ($tp$)  & false positive ($fp$) \\
		$\hat{y}\neq{A}$ & false negative ($fn$) & true negative ($tn$)  \\
		\hline
	\end{tabular}
	\caption{Confusion Matrix for the binary case of some class label, $A$. $y$ represents the true label while $\hat{y}$ represents the predicted label.}
	\label{table:confusion_matrix}
\end{table}

Two commonly extrapolated metrics from which other measures are typically derived are precision and recall. Given some class $A$, the former is a ratio of correctly labelled instances to all instances labelled $A$ whereas the latter compares the amount of correctly labelled instances to all instances of class $A$ present in the data, which is analogous to the accuracy for class $A$. Formally, based on the definitions in table \ref{table:confusion_matrix}, the two measures are given by:

\begin{equation} \label{eq:precision}
	precision_A = \frac{tp_A}{tp_A + fp_A}
\end{equation}
\begin{equation} \label{eq:recall}
	recall_A = \frac{tp_A}{tp_A + fn_A}
\end{equation}

For the case with $C$ classes, the total number of instances for a class $c$, $N_c$ is equal to $tp_c + fn_c$. The prevalent metric for accuracy that is reported in the literature, equivalent to the micro-averaged recall, is thus computed by:

\begin{equation} \label{eq:accuracy}
	accuracy = \frac{\sum_{c}^{C}tp_c}{N} = recall^{micro}
\end{equation}

Where $N$ is the total number of instances in the data. Using this micro-averaged metric, however, is not necessarily the most accurate indicator of a model's performance in a classification task, particularly when the dataset that is being utilized is heavily biased to one specific class. Care must be given in the training phase of any machine learning model to ensure that the model is exposed to all classes in question in a balanced way. This is because in computing the micro-average, the weighting scheme is distributed across all instances in the dataset, as opposed to the classes. A more sophisticated metric that is robust to this issue is the macro-averaged F1-score which equally distributes the weight across all classes as opposed to instances \cite{manning2010}. The macro-averaged F-measure is given by the harmonic mean of the, macro-averaged, precision and recall for a specific class. For some class $A$, this is given by:

\begin{equation} \label{eq:f1_measure}
	F1^{macro}_{A} = \frac{2P^{macro}_{A}R^{macro}_{A}}{P^{macro}_{A}+R^{macro}_{A}}
\end{equation}

Training a model heavily on one specific class, or not enough on another could lead the model to classify the majority of test samples to the biased class or being unable to correctly classify the class that has been under-represented in training, since the model would not have gathered enough information to discern this class. In the case where the testing dataset would be imbalanced towards the same class, the overall accuracy would lack the sufficient information expected as a metric to illustrate the effectiveness of the model to classify samples into the correct class since the model would have been trained in a biased way towards the class that is prevalent.

As an example, a frequently cited benchmark dataset is presented in \cite{dong}, this dataset consists of 6248 training and 692 test phrases collected from twitter, each annotated with a particular sentiment (negative, neutral or positive) towards a specific target that appears in the tweet. Within both the training and testing subsets of this dataset, there are twice as many neutral instances as there are positive and negative instances. Works such as Chen et al. \cite{chen2017} and Dong et al. \cite{dong}, correctly point out the shortcoming of accuracy as a valid performance metric in situations such as this, and cite macro-averaged F1 scores in their results.

\subsection{Manual Feature Engineering}
Initially, the conventional approach involved manually extracting the most expressive and information rich features from sentences that would subsequently be processed through some statistical model such as a Support Vector Machine (SVM) for classification.

This entailed the formulation of processes by which to obtain these features, and was normally preceded by some form of normalization of the original data before these features could be extracted. Typically many types of these features were used in conjunction, each intended to extrapolate differing particularities about a specific aspect of the text, such as whether a specific token represented a noun or an adjective, or details about the words surrounding it.

The capacity of the SVM had been demonstrated on the general task of sentiment analysis in works such as Pang et al. \cite{pang2002}, as well as other tools such as, bag-of-words, part-of-speech tags and other morphological and syntactical features and external resources such as linguistic parsers and sentiment lexicon, employed in works such as Dong et al. \cite{dong}, Vo et al. \cite{vo2015} and Nguyen et al. \cite{nguyen2015}.

However, as \cite{tang2016b} point out, these methods would implicitly impose an external dependency on the system. Moreover, within the context of social media, where conventional rules of language are often times regarded rather as guidelines, various studies question the applicability of dependency parsing techniques that rely on a particular degree of formality, or structure, within the content itself \cite{tang2016b}, \cite{chen2017}. Nevertheless these features have proven their worth when used in conjunction with powerful models such as the aforementioned SVM \cite{kiritchenko} \cite{wagner2014}, as well as neural networks \cite{dong}, \cite{vo2015}, in predicting sentiment polarity.

Even in the work that followed, focusing on increasingly autonomous feature extraction methods and more sophisticated deep learning architectures such as the Long Short Term Memory (LSTM) model, \cite{tang2016b} make note of the competitive results obtained by the SVM approach in \cite{kiritchenko} when compared to their implementations.

\subsubsection{Drawbacks}
Although works such as \cite{kiritchenko}, \cite{wagner2014} obtained encouraging results, much of the subsequent literature recognizes that these results were exceedingly contingent on the choice of features \cite{tang2016b}.

Although the manual feature-based approach fared well in their work, Tang et al. \cite{tang2016b} suggest that features of this kind lack the required resolution of detail that would accurately capture the interplay between target and context. The features that had been used had sound rationales behind them, however devising these rationales was in itself becoming increasingly time-consuming. One reason for this is scalability; with the increase of data available, more considerations and specifics must be accounted for when manually devising these features.

As alluded to by Zheng et al. \cite{zheng2018}, with the aforementioned increase in labor involved, these approaches could be regarded as a bottleneck in terms of performance of these models and the wealth of data available. A more autonomous solution that would accurately capture the intricacies of language from an expansive wealth of text at a deeper level, not contingent on a proportionally large amount of labor-intensive manual feature-engineering, was desired to further advance TSA.
\end{document}
