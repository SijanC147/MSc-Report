% !TEX root = ../../fyp.tex
\documentclass[../../fyp.tex]{subfiles}

\begin{document}
\subsection{Model Graph}
The model graph provides an intuitive way of visualizing the architecture and data flow of a model. Nodes are used to represent single operations or groups thereof, and connections represent the tensor data that flows from one operation to the next.\footnote{https://www.tensorflow.org/guide/graph\_viz} 

\subsection{Histograms}
Histograms illustrate the evolution of tensor distributions with training. Internally, the framework attempts to produce histograms for all tensors that are trained.\footnote{https://www.tensorflow.org/guide/tensorboard\_histograms}

\subsection{Embedding Projections}
When preparing feature data for an experiment, the framework also produces a tab-separated-value (tsv) file listing all the tokens, including any OOV buckets, in the order they appear in the embedding. This file can be used in conjunction with a Tensorboard projector tool to attach labels to points of the embedding matrix projected in $\mathbb{R}^3$ space using principal component analysis (PCA). This may provide additional insights into how the representation of OOV buckets evolves with respect to other tokens by visualizing neighboring points.
%TODO Include an example image of embedding visualized in this way, possibly showing nearest neighbors of OOV buckets

\subsection{Confusion Matrix}
Matplotlib\footnote{https://matplotlib.org/} was used to render most of the visualizations offered by the framework for its extensive tool-set. The confusion matrix is generated using a custom-built hook and provides a high-level overview of the model performance. It is rendered as a heat-map spread across all samples of the evaluation dataset. Although this information may be condensed into informative scalar metrics, such as the macro-F1, the confusion matrix may convey more insight into the per-class transient performance of the model. Ideally, with training, the heat-map should consolidate across the diagonal of the matrix.
%TODO Include example image of multiple confusion matrices over time showing how the heatmap distribution changes over time

\subsection{Dataset Distribution Figure}
Two pie-charts are generated for the training and test datasets which depict the class distribution of the each dataset. Moreover, when merged datasets are used, the contribution of each constituent dataset and their class distributions are also included in this figure. This type of visualization intuitively highlights class imbalances as well as their sources while also efficiently conveying the contribution of each dataset relative to the others in a merged dataset.
%TODO Include image of a dataset distribution piechart

\subsection{Attention Heat-maps}
Attention weight vectors are obtained using custom implementations of attention units which expose these data and their corresponding string tokens. These are used by the corresponding add-on to generate attention heat-maps for a subset of samples from each batch during evaluation. The size of this subset can be adjusted using the auxiliary-configuration parameters to avoid excessive heat-maps from being generated for increasingly large datasets. 

The attention heat-maps use a color-map to contrast the different weights being placed on each token. Over time, as the model learns to distinguish which tokens maximally contribute to the sentiment of a sentence, they are expected to be highlighted in contrast to their surroundings which signifies a higher attention weight. Two attention heat-maps are generated for each sample, the first illustrating the color-coded attention weight distribution corresponding to each string token and a second, supplementing the first, with the precise real-valued weights.

For models that employ multiple \enquote{hops}, such as memory networks, the process is repeated for each hop. These models are expected to concentrate their attention on the most salient tokens with each hop, as well as improving the process on the whole with each evaluation checkpoint.
%TODO Include two images of attention maps one for the normal case showing both generated types and one for memory networks showing multiple hops
\end{document}